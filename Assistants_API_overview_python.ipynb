{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucaokylong/LLM_learning/blob/main/Assistants_API_overview_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0efsupZIAvl"
      },
      "source": [
        "# Assistants API Overview (Python SDK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hY5ihRzIAvn"
      },
      "source": [
        "## API Chat Completions so với API Assistants\n",
        "\n",
        "Các thành phần cơ bản của **API Chat Completions** là `Messages`, trên đó bạn thực hiện một `Completion` với một `Model` (`gpt-3.5-turbo`, `gpt-4`, v.v.). Nó nhẹ và mạnh mẽ, nhưng vốn dĩ không có trạng thái, có nghĩa là bạn phải quản lý trạng thái cuộc hội thoại, định nghĩa công cụ, tài liệu truy xuất và thực thi mã theo cách thủ công.\n",
        "\n",
        "Các thành phần cơ bản của **API Assistants** là\n",
        "\n",
        "- `Assistants`, bao gồm một mô hình cơ sở, các hướng dẫn, công cụ và tài liệu (ngữ cảnh),\n",
        "- `Threads`, đại diện cho trạng thái của một cuộc hội thoại, và\n",
        "- `Runs`, thực thi một `Assistant` trên một `Thread`, bao gồm các phản hồi dạng văn bản và sử dụng công cụ nhiều bước.\n",
        "\n",
        "Chúng ta sẽ xem xét cách những thành phần này có thể được sử dụng để tạo ra các trải nghiệm mạnh mẽ, có trạng thái.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw2FEBUWIAvn"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Python SDK\n",
        "\n",
        "> **Note**\n",
        "> We've updated our [Python SDK](https://github.com/openai/openai-python) to add support for the Assistants API, so you'll need to update it to the latest version (`1.2.3` at time of writing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgBVvTk0IAvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b7d7cb-b333-4a1e-c90f-302473631ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-BdAGcUIAvo"
      },
      "source": [
        "And make sure it's up to date by running:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP5ANcsyIAvo",
        "outputId": "4f1f541f-5684-4e73-d2e7-96caed115b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version: 1.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip show openai | grep Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpm_hSwBIAvp"
      },
      "source": [
        "### Pretty Printing Helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JCZED11IAvp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def show_json(obj):\n",
        "    display(json.loads(obj.model_dump_json()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk9Oii8KIAvp"
      },
      "source": [
        "## Complete Example with Assistants API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkrvzuemIAvp"
      },
      "source": [
        "### Assistants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BeaGKTQIAvp"
      },
      "source": [
        "You can also create Assistants directly through the Assistants API, like so:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o03HFmyqIAvp",
        "outputId": "e15cc7cd-58e5-4ebd-9842-7ff5aecece33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'created_at': 1710933670,\n",
              " 'description': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'name': 'Math Tutor',\n",
              " 'object': 'assistant',\n",
              " 'tools': []}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Math Tutor\",\n",
        "    instructions=\"You are a personal math tutor. Answer questions briefly, in a sentence or less.\",\n",
        "    model=\"gpt-3.5-turbo\",\n",
        ")\n",
        "show_json(assistant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADeBbm-5IAvp"
      },
      "source": [
        "Regardless of whether you create your Assistant through the Dashboard or with the API, you'll want to keep track of the Assistant ID. This is how you'll refer to your Assistant throughout Threads and Runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LliVSGiPIAvp"
      },
      "source": [
        "Next, we'll create a new Thread and add a Message to it. This will hold the state of our conversation, so we don't have re-send the entire message history each time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQIsCHyOIAvp"
      },
      "source": [
        "### Threads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGnP-GpkIAvq"
      },
      "source": [
        "Create a new thread:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urucbn1HIAvq",
        "outputId": "87cac81a-c55a-4e7b-f8e5-98617525c367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'thread_CvLO0Vou6c1yCHqOJXHqsBeo',\n",
              " 'created_at': 1710933682,\n",
              " 'metadata': {},\n",
              " 'object': 'thread'}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "thread = client.beta.threads.create()\n",
        "show_json(thread)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rTCAQMBIAvq"
      },
      "source": [
        "Then add the Message to the thread:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OroKArtIAvq",
        "outputId": "b2c34e21-f16b-42d5-fc12-01fcd552401b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'msg_FzkHxehXmub1ucR7mFSHJTNJ',\n",
              " 'assistant_id': None,\n",
              " 'completed_at': None,\n",
              " 'content': [{'text': {'annotations': [],\n",
              "    'value': 'I need to solve the equation `3x + 11 = 14`. Can you help me?'},\n",
              "   'type': 'text'}],\n",
              " 'created_at': 1710933689,\n",
              " 'file_ids': [],\n",
              " 'incomplete_at': None,\n",
              " 'incomplete_details': None,\n",
              " 'metadata': {},\n",
              " 'object': 'thread.message',\n",
              " 'role': 'user',\n",
              " 'run_id': None,\n",
              " 'status': None,\n",
              " 'thread_id': 'thread_CvLO0Vou6c1yCHqOJXHqsBeo'}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n",
        ")\n",
        "show_json(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQhR42NJIAvq"
      },
      "source": [
        "> **Note**\n",
        "> Even though you're no longer sending the entire history each time, you will still be charged for the tokens of the entire conversation history with each Run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2z8wtXLIAvq"
      },
      "source": [
        "### Runs\n",
        "\n",
        "Notice how the Thread we created is **not** associated with the Assistant we created earlier! Threads exist independently from Assistants, which may be different from what you'd expect if you've used ChatGPT (where a thread is tied to a model/GPT).\n",
        "\n",
        "To get a completion from an Assistant for a given Thread, we must create a Run. Creating a Run will indicate to an Assistant it should look at the messages in the Thread and take action: either by adding a single response, or using tools.\n",
        "\n",
        "> **Note**\n",
        "> Runs are a key difference between the Assistants API and Chat Completions API. While in Chat Completions the model will only ever respond with a single message, in the Assistants API a Run may result in an Assistant using one or multiple tools, and potentially adding multiple messages to the Thread.\n",
        "\n",
        "To get our Assistant to respond to the user, let's create the Run. As mentioned earlier, you must specify _both_ the Assistant and the Thread.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwiCCM7vIAvq",
        "outputId": "9f7fa7e2-9141-4587-935d-133fee6081ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'run_F2mOduFP47x8PyRVm9BT2VO3',\n",
              " 'assistant_id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': None,\n",
              " 'created_at': 1710933731,\n",
              " 'expires_at': 1710934331,\n",
              " 'failed_at': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'last_error': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': None,\n",
              " 'started_at': None,\n",
              " 'status': 'queued',\n",
              " 'thread_id': 'thread_CvLO0Vou6c1yCHqOJXHqsBeo',\n",
              " 'tools': [],\n",
              " 'usage': None}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")\n",
        "show_json(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrQgWSWJIAvq"
      },
      "source": [
        "Unlike creating a completion in the Chat Completions API, **creating a Run is an asynchronous operation**. It will return immediately with the Run's metadata, which includes a `status` that will initially be set to `queued`. The `status` will be updated as the Assistant performs operations (like using tools and adding messages).\n",
        "\n",
        "To know when the Assistant has completed processing, we can poll the Run in a loop. (Support for streaming is coming soon!) While here we are only checking for a `queued` or `in_progress` status, in practice a Run may undergo a [variety of status changes](https://platform.openai.com/docs/api-reference/runs/object#runs/object-status) which you can choose to surface to the user. (These are called Steps, and will be covered later.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2pvymjNIAvq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_on_run(run, thread):\n",
        "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "        )\n",
        "        time.sleep(0.5)\n",
        "    return run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGcFg5E5IAvq",
        "outputId": "740ddea0-3fd1-427a-ed0d-47ec27af217c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'run_F2mOduFP47x8PyRVm9BT2VO3',\n",
              " 'assistant_id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': 1710933732,\n",
              " 'created_at': 1710933731,\n",
              " 'expires_at': None,\n",
              " 'failed_at': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'last_error': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': None,\n",
              " 'started_at': 1710933731,\n",
              " 'status': 'completed',\n",
              " 'thread_id': 'thread_CvLO0Vou6c1yCHqOJXHqsBeo',\n",
              " 'tools': [],\n",
              " 'usage': {'completion_tokens': 32, 'prompt_tokens': 48, 'total_tokens': 80}}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wait_on_run(run, thread)\n",
        "show_json(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4trABNL3IAvq"
      },
      "source": [
        "### Messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zIVioF7IAvq"
      },
      "source": [
        "Now that the Run has completed, we can list the Messages in the Thread to see what got added by the Assistant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEK__6xhIAvr",
        "outputId": "232f4507-86d1-4322-fcf6-0e0bc48c8ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'data': [{'id': 'msg_1sFioGtpvSuSR6bMy78Bie3x',\n",
              "   'assistant_id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              "   'completed_at': None,\n",
              "   'content': [{'text': {'annotations': [],\n",
              "      'value': 'Sure! Subtract 11 from both sides to get `3x = 3`, then divide by 3 to find `x = 1`.'},\n",
              "     'type': 'text'}],\n",
              "   'created_at': 1710933732,\n",
              "   'file_ids': [],\n",
              "   'incomplete_at': None,\n",
              "   'incomplete_details': None,\n",
              "   'metadata': {},\n",
              "   'object': 'thread.message',\n",
              "   'role': 'assistant',\n",
              "   'run_id': 'run_F2mOduFP47x8PyRVm9BT2VO3',\n",
              "   'status': None,\n",
              "   'thread_id': 'thread_CvLO0Vou6c1yCHqOJXHqsBeo'},\n",
              "  {'id': 'msg_FzkHxehXmub1ucR7mFSHJTNJ',\n",
              "   'assistant_id': None,\n",
              "   'completed_at': None,\n",
              "   'content': [{'text': {'annotations': [],\n",
              "      'value': 'I need to solve the equation `3x + 11 = 14`. Can you help me?'},\n",
              "     'type': 'text'}],\n",
              "   'created_at': 1710933689,\n",
              "   'file_ids': [],\n",
              "   'incomplete_at': None,\n",
              "   'incomplete_details': None,\n",
              "   'metadata': {},\n",
              "   'object': 'thread.message',\n",
              "   'role': 'user',\n",
              "   'run_id': None,\n",
              "   'status': None,\n",
              "   'thread_id': 'thread_CvLO0Vou6c1yCHqOJXHqsBeo'}],\n",
              " 'object': 'list',\n",
              " 'first_id': 'msg_1sFioGtpvSuSR6bMy78Bie3x',\n",
              " 'last_id': 'msg_FzkHxehXmub1ucR7mFSHJTNJ',\n",
              " 'has_more': False}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "show_json(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kU8a-ykIAvr"
      },
      "source": [
        "As you can see, Messages are ordered in reverse-chronological order – this was done so the most recent results are always on the first `page` (since results can be paginated). Do keep a look out for this, since this is the opposite order to messages in the Chat Completions API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIZ-axGcIAvr"
      },
      "source": [
        "Let's ask our Assistant to explain the result a bit further!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzN6MpY-IAvr",
        "outputId": "da29dee7-7619-4e33-cb4b-8a019cab001c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'data': [{'id': 'msg_sWF1P2iCOlqgnJjK8EfPeImP',\n",
              "   'assistant_id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              "   'completed_at': None,\n",
              "   'content': [{'text': {'annotations': [],\n",
              "      'value': 'Certainly! To solve the equation `3x + 11 = 14`, you first isolate the term with the variable by subtracting 11 from both sides, then divide by the coefficient of x to find its value.'},\n",
              "     'type': 'text'}],\n",
              "   'created_at': 1710933830,\n",
              "   'file_ids': [],\n",
              "   'incomplete_at': None,\n",
              "   'incomplete_details': None,\n",
              "   'metadata': {},\n",
              "   'object': 'thread.message',\n",
              "   'role': 'assistant',\n",
              "   'run_id': 'run_OJ2NNZlRxcz1OLDxnossxxVP',\n",
              "   'status': None,\n",
              "   'thread_id': 'thread_CvLO0Vou6c1yCHqOJXHqsBeo'}],\n",
              " 'object': 'list',\n",
              " 'first_id': 'msg_sWF1P2iCOlqgnJjK8EfPeImP',\n",
              " 'last_id': 'msg_sWF1P2iCOlqgnJjK8EfPeImP',\n",
              " 'has_more': False}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a message to append to our thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id, role=\"user\", content=\"Could you explain this to me?\"\n",
        ")\n",
        "\n",
        "# Execute our run\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "# Wait for completion\n",
        "wait_on_run(run, thread)\n",
        "\n",
        "# Retrieve all the messages added after our last user message\n",
        "messages = client.beta.threads.messages.list(\n",
        "    thread_id=thread.id, order=\"asc\", after=message.id\n",
        ")\n",
        "show_json(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNEb5G8lIAvs"
      },
      "source": [
        "This may feel like a lot of steps to get a response back, especially for this simple example. However, you'll soon see how we can add very powerful functionality to our Assistant without changing much code at all!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz2msjxgIAvs"
      },
      "source": [
        "### Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezw7t0OIAvs"
      },
      "source": [
        "Let's take a look at how we could potentially put all of this together. Below is all the code you need to use an Assistant you've created.\n",
        "\n",
        "Since we've already created our Math Assistant, I've saved its ID in `MATH_ASSISTANT_ID`. I then defined two functions:\n",
        "\n",
        "- `submit_message`: create a Message on a Thread, then start (and return) a new Run\n",
        "- `get_response`: returns the list of Messages in a Thread\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmElZS12IAvs"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "MATH_ASSISTANT_ID = assistant.id  # or a hard-coded ID like \"asst-...\"\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def submit_message(assistant_id, thread, user_message):\n",
        "    client.beta.threads.messages.create(\n",
        "        thread_id=thread.id, role=\"user\", content=user_message\n",
        "    )\n",
        "    return client.beta.threads.runs.create(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=assistant_id,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_response(thread):\n",
        "    return client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taFpchkmIAvs"
      },
      "source": [
        "I've also defined a `create_thread_and_run` function that I can re-use (which is actually almost identical to the [`client.beta.threads.create_and_run`](https://platform.openai.com/docs/api-reference/runs/createThreadAndRun) compound function in our API ;) ). Finally, we can submit our mock user requests each to a new Thread.\n",
        "\n",
        "Notice how all of these API calls are asynchronous operations; this means we actually get async behavior in our code without the use of async libraries! (e.g. `asyncio`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kyDlRxIIAvs"
      },
      "outputs": [],
      "source": [
        "id = MATH_ASSISTANT_ID\n",
        "\n",
        "def create_thread_and_run(user_input, id):\n",
        "    thread = client.beta.threads.create()\n",
        "    run = submit_message(id, thread, user_input)\n",
        "    return thread, run\n",
        "\n",
        "\n",
        "# Emulating concurrent user requests\n",
        "thread1, run1 = create_thread_and_run(\n",
        "    \"I need to solve the equation `3x + 11 = 14`. Can you help me?\",id\n",
        ")\n",
        "thread2, run2 = create_thread_and_run(\"Could you explain linear algebra to me?\",id)\n",
        "thread3, run3 = create_thread_and_run(\"I don't like math. What can I do?\",id)\n",
        "\n",
        "# Now all Runs are executing..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR0QW_bSIAvs"
      },
      "source": [
        "Once all Runs are going, we can wait on each and get the responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GofIZ9ijIAvs",
        "outputId": "c32f82c6-f97c-4594-a484-73c4afa9a092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Messages\n",
            "user: I need to solve the equation `3x + 11 = 14`. Can you help me?\n",
            "assistant: Sure! Subtract 11 from both sides to get `3x = 3`, then divide by 3 to find `x = 1`.\n",
            "\n",
            "# Messages\n",
            "user: Could you explain linear algebra to me?\n",
            "assistant: Linear algebra is a branch of mathematics that focuses on the study of vectors, vector spaces, and linear transformations.\n",
            "\n",
            "# Messages\n",
            "user: I don't like math. What can I do?\n",
            "assistant: Try to find the fun in math by solving real-life problems or playing math-related games.\n",
            "\n",
            "# Messages\n",
            "user: I don't like math. What can I do?\n",
            "assistant: Try to find the fun in math by solving real-life problems or playing math-related games.\n",
            "user: Thank you!\n",
            "assistant: You're welcome! If you have any more questions, feel free to ask.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Pretty printing helper\n",
        "def pretty_print(messages):\n",
        "    print(\"# Messages\")\n",
        "    for m in messages:\n",
        "        print(f\"{m.role}: {m.content[0].text.value}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Waiting in a loop\n",
        "def wait_on_run(run, thread):\n",
        "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "        )\n",
        "        time.sleep(0.5)\n",
        "    return run\n",
        "\n",
        "\n",
        "# Wait for Run 1\n",
        "run1 = wait_on_run(run1, thread1)\n",
        "pretty_print(get_response(thread1))\n",
        "\n",
        "# Wait for Run 2\n",
        "run2 = wait_on_run(run2, thread2)\n",
        "pretty_print(get_response(thread2))\n",
        "\n",
        "# Wait for Run 3\n",
        "run3 = wait_on_run(run3, thread3)\n",
        "pretty_print(get_response(thread3))\n",
        "\n",
        "# Thank our assistant on Thread 3 :)\n",
        "run4 = submit_message(MATH_ASSISTANT_ID, thread3, \"Thank you!\")\n",
        "run4 = wait_on_run(run4, thread3)\n",
        "pretty_print(get_response(thread3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDVtNR3nIAvs"
      },
      "source": [
        "Et voilà!\n",
        "\n",
        "You may have noticed that this code is not actually specific to our math Assistant at all... this code will work for any new Assistant you create simply by changing the Assistant ID! That is the power of the Assistants API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XofXGre7IAvs"
      },
      "source": [
        "## Tools\n",
        "\n",
        "A key feature of the Assistants API is the ability to equip our Assistants with Tools, like Code Interpreter, Retrieval, and custom Functions. Let's take a look at each.\n",
        "\n",
        "### Code Interpreter\n",
        "\n",
        "Let's equip our Math Tutor with the [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter) tool, which we can do from the Dashboard...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWTzZ8MHIAvt"
      },
      "source": [
        "...or the API, using the Assistant ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1pQ2SAyIAvt",
        "outputId": "9200aade-8f3d-4f77-d05d-51777f9c358a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'created_at': 1710933670,\n",
              " 'description': None,\n",
              " 'file_ids': ['file-nfw91nJE2kbVTVHJCKBxBmqx'],\n",
              " 'instructions': 'You are a math tutor.',\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'name': 'Math Tutor',\n",
              " 'object': 'assistant',\n",
              " 'tools': [{'type': 'code_interpreter'}]}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "assistant = client.beta.assistants.update(\n",
        "    MATH_ASSISTANT_ID,\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        ")\n",
        "show_json(assistant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHbwqAP4IAvt"
      },
      "source": [
        "Now, let's ask the Assistant to use its new tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y15Vm3VqIAvt",
        "outputId": "c2335b60-2cc9-493c-e9e1-8cf1c8f11627",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Messages\n",
            "user: Generate the first 20 fibbonaci numbers with code.\n",
            "assistant: I have generated the first 20 Fibonacci numbers. Here they are:\n",
            "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "thread, run = create_thread_and_run(\n",
        "    \"Generate the first 20 fibbonaci numbers with code.\", id = MATH_ASSISTANT_ID\n",
        ")\n",
        "run = wait_on_run(run, thread)\n",
        "pretty_print(get_response(thread))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swS5CLZbIAvt"
      },
      "source": [
        "And that's it! The Assistant used Code Interpreter in the background, and gave us a final response.\n",
        "\n",
        "For some use cases this may be enough – however, if we want more details on what precisely an Assistant is doing we can take a look at a Run's Steps.\n",
        "\n",
        "### Steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n90DHGiIAvt"
      },
      "source": [
        "A Run is composed of one or more Steps. Like a Run, each Step has a `status` that you can query. This is useful for surfacing the progress of a Step to a user (e.g. a spinner while the Assistant is writing code or performing retrieval).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z43AlAvtIAvt"
      },
      "outputs": [],
      "source": [
        "run_steps = client.beta.threads.runs.steps.list(\n",
        "    thread_id=thread.id, run_id=run.id, order=\"asc\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_steps.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga6ntYq9njrM",
        "outputId": "78e03191-965d-4a27-9ab9-c54f9b9c0b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[RunStep(id='step_GCXlwEfHT2s0yksZfswhBmk9', assistant_id='asst_wIiQdreLlLNzoud1MxL2iheM', cancelled_at=None, completed_at=1710936027, created_at=1710936024, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_WSG2j8XD37GlXTNaW5Pq9Met', status='completed', step_details=ToolCallsStepDetails(tool_calls=[CodeInterpreterToolCall(id='call_oHTHdOm1w7tT8ZYg1k9zwhs0', code_interpreter=CodeInterpreter(input='# Function to generate the first n Fibonacci numbers\\ndef generate_fibonacci(n):\\n    fibonacci_numbers = [0, 1]\\n    for i in range(2, n):\\n        next_number = fibonacci_numbers[-1] + fibonacci_numbers[-2]\\n        fibonacci_numbers.append(next_number)\\n    return fibonacci_numbers\\n\\n# Generate the first 20 Fibonacci numbers\\nfirst_20_fibonacci = generate_fibonacci(20)\\nfirst_20_fibonacci', outputs=[CodeInterpreterOutputLogs(logs='[0,\\n 1,\\n 1,\\n 2,\\n 3,\\n 5,\\n 8,\\n 13,\\n 21,\\n 34,\\n 55,\\n 89,\\n 144,\\n 233,\\n 377,\\n 610,\\n 987,\\n 1597,\\n 2584,\\n 4181]', type='logs')]), type='code_interpreter')], type='tool_calls'), thread_id='thread_wuziOnSJ0RSkA89Q7G5NUVE0', type='tool_calls', usage=Usage(completion_tokens=99, prompt_tokens=175, total_tokens=274), expires_at=None),\n",
              " RunStep(id='step_lKx7lUeT8GxZ4eVzaoSrsI3H', assistant_id='asst_wIiQdreLlLNzoud1MxL2iheM', cancelled_at=None, completed_at=1710936029, created_at=1710936027, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_WSG2j8XD37GlXTNaW5Pq9Met', status='completed', step_details=MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_amss3bqm01dOsCqUhZpfpJo3'), type='message_creation'), thread_id='thread_wuziOnSJ0RSkA89Q7G5NUVE0', type='message_creation', usage=Usage(completion_tokens=79, prompt_tokens=345, total_tokens=424), expires_at=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9dRNVhIAvt"
      },
      "source": [
        "Let's take a look at each Step's `step_details`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ5WFEdMIAvt",
        "outputId": "48cec6ac-8f96-45be-ffdd-de8b67e2c838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'tool_calls': [{'id': 'call_WQNwrTIpqNApEfAbhre6tIVD',\n",
              "   'code_interpreter': {'input': 'def fibonacci(n):\\n    fib_seq = [0, 1]\\n    for i in range(2, n):\\n        fib_seq.append(fib_seq[i-1] + fib_seq[i-2])\\n    return fib_seq\\n\\n# Generate the first 20 Fibonacci numbers\\nfibonacci(20)',\n",
              "    'outputs': [{'logs': '[0,\\n 1,\\n 1,\\n 2,\\n 3,\\n 5,\\n 8,\\n 13,\\n 21,\\n 34,\\n 55,\\n 89,\\n 144,\\n 233,\\n 377,\\n 610,\\n 987,\\n 1597,\\n 2584,\\n 4181]',\n",
              "      'type': 'logs'}]},\n",
              "   'type': 'code_interpreter'}],\n",
              " 'type': 'tool_calls'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "null\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'message_creation': {'message_id': 'msg_g5buy2IXPziCsoH0Y3cG38IL'},\n",
              " 'type': 'message_creation'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "null\n"
          ]
        }
      ],
      "source": [
        "for step in run_steps.data:\n",
        "    step_details = step.step_details\n",
        "    print(json.dumps(show_json(step_details), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8SAnGZ3IAvt"
      },
      "source": [
        "We can see the `step_details` for two Steps:\n",
        "\n",
        "1. `tool_calls` (plural, since it could be more than one in a single Step)\n",
        "2. `message_creation`\n",
        "\n",
        "The first Step is a `tool_calls`, specifically using the `code_interpreter` which contains:\n",
        "\n",
        "- `input`, which was the Python code generated before the tool was called, and\n",
        "- `output`, which was the result of running the Code Interpreter.\n",
        "\n",
        "The second Step is a `message_creation`, which contains the `message` that was added to the Thread to communicate the results to the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHzJgFGTIAvu"
      },
      "source": [
        "### Retrieval\n",
        "\n",
        "Another powerful tool in the Assistants API is [Retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval): the ability to upload files that the Assistant will use as a knowledge base when answering questions. This can also be enabled from the Dashboard or the API, where we can upload files we want to be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxLHoFe8IAvu"
      },
      "source": [
        "![Enabling retrieval](../images/assistants_overview_enable_retrieval.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwaE5hlyIAvu",
        "outputId": "71cec5e9-2f69-4f36-b35f-56a8f5c018d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'created_at': 1710933670,\n",
              " 'description': None,\n",
              " 'file_ids': ['file-u5H0QWd95vzYHIEmJfR1Vn3E'],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'name': 'Math Tutor',\n",
              " 'object': 'assistant',\n",
              " 'tools': [{'type': 'code_interpreter'}, {'type': 'retrieval'}]}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Upload the file\n",
        "file = client.files.create(\n",
        "    file=open(\n",
        "        \"/content/language_models_are_unsupervised_multitask_learners.pdf\",\n",
        "        \"rb\",\n",
        "    ),\n",
        "    purpose=\"assistants\",\n",
        ")\n",
        "# Update Assistant\n",
        "assistant = client.beta.assistants.update(\n",
        "    MATH_ASSISTANT_ID,\n",
        "    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"retrieval\"}],\n",
        "    file_ids=[file.id],\n",
        ")\n",
        "show_json(assistant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8l16-MIAvu",
        "outputId": "f20c3efa-6f17-4d5e-d781-58c86dbddc9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Messages\n",
            "user: What are some cool math concepts behind this ML paper pdf? Explain in two sentences.\n",
            "assistant: This paper explores the concept of language models being trained on large amounts of text data to perform various natural language processing tasks without explicit supervision, showcasing the potential of zero-shot task transfer by conditioning on both input and task. The approach leverages the capacity of language models to improve performance across tasks in a log-linear fashion, demonstrating the importance of multitask learning and generalization in machine learning systems.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "thread, run = create_thread_and_run(\n",
        "    \"What are some cool math concepts behind this ML paper pdf? Explain in two sentences.\", id = MATH_ASSISTANT_ID\n",
        ")\n",
        "run = wait_on_run(run, thread)\n",
        "pretty_print(get_response(thread))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_steps = client.beta.threads.runs.steps.list(\n",
        "    thread_id=thread.id, run_id=run.id, order=\"asc\"\n",
        ")\n",
        "for step in run_steps.data:\n",
        "    step_details = step.step_details\n",
        "    print(json.dumps(show_json(step_details), indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "QcZwpZbHoKtz",
        "outputId": "d5abb951-0d55-473f-f351-bfffb91586dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'tool_calls': [{'id': 'call_4eoK0vY3G3YNMih7MWg9Zpp1',\n",
              "   'retrieval': {},\n",
              "   'type': 'retrieval'}],\n",
              " 'type': 'tool_calls'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "null\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'message_creation': {'message_id': 'msg_QNaNvRvLWiJsHAoedRlRTbxq'},\n",
              " 'type': 'message_creation'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYsdA-s2IAvu"
      },
      "source": [
        "> **Note**\n",
        "> There are more intricacies in Retrieval, like [Annotations](https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages), which may be covered in another cookbook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n_GOu9qIAvu"
      },
      "source": [
        "### Functions\n",
        "\n",
        "As a final powerful tool for your Assistant, you can specify custom [Functions](https://platform.openai.com/docs/assistants/tools/function-calling) (much like the [Function Calling](https://platform.openai.com/docs/guides/function-calling) in the Chat Completions API). During a Run, the Assistant can then indicate it wants to call one or more functions you specified. You are then responsible for calling the Function, and providing the output back to the Assistant.\n",
        "\n",
        "Let's take a look at an example by defining a `display_quiz()` Function for our Math Tutor.\n",
        "\n",
        "This function will take a `title` and an array of `question`s, display the quiz, and get input from the user for each:\n",
        "\n",
        "- `title`\n",
        "- `questions`\n",
        "  - `question_text`\n",
        "  - `question_type`: [`MULTIPLE_CHOICE`, `FREE_RESPONSE`]\n",
        "  - `choices`: [\"choice 1\", \"choice 2\", ...]\n",
        "\n",
        "Unfortunately I don't know how to get user input within a Python Notebook, so I'll be mocking out responses with `get_mock_response...`. This is where you'd get the user's actual input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjjqzncjIAvu"
      },
      "outputs": [],
      "source": [
        "def get_mock_response_from_user_multiple_choice():\n",
        "    return \"a\"\n",
        "\n",
        "\n",
        "def get_mock_response_from_user_free_response():\n",
        "    return \"I don't know.\"\n",
        "\n",
        "\n",
        "def display_quiz(title, questions):\n",
        "    print(\"Quiz:\", title)\n",
        "    print()\n",
        "    responses = []\n",
        "\n",
        "    for q in questions:\n",
        "        print(q[\"question_text\"])\n",
        "        response = \"\"\n",
        "\n",
        "        # If multiple choice, print options\n",
        "        if q[\"question_type\"] == \"MULTIPLE_CHOICE\":\n",
        "            for i, choice in enumerate(q[\"choices\"]):\n",
        "                print(f\"{i}. {choice}\")\n",
        "            response = get_mock_response_from_user_multiple_choice()\n",
        "\n",
        "        # Otherwise, just get response\n",
        "        elif q[\"question_type\"] == \"FREE_RESPONSE\":\n",
        "            response = get_mock_response_from_user_free_response()\n",
        "\n",
        "        responses.append(response)\n",
        "        print()\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv90QFnYIAvu"
      },
      "source": [
        "Here's what a sample quiz would look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr-S5e9HIAvu",
        "outputId": "e9188525-a90c-4c8d-bceb-aaaaf363dc84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quiz: Sample Quiz\n",
            "\n",
            "What is your name?\n",
            "\n",
            "What is your favorite color?\n",
            "0. Red\n",
            "1. Blue\n",
            "2. Green\n",
            "3. Yellow\n",
            "\n",
            "Responses: [\"I don't know.\", 'a']\n"
          ]
        }
      ],
      "source": [
        "responses = display_quiz(\n",
        "    \"Sample Quiz\",\n",
        "    [\n",
        "        {\"question_text\": \"What is your name?\", \"question_type\": \"FREE_RESPONSE\"},\n",
        "        {\n",
        "            \"question_text\": \"What is your favorite color?\",\n",
        "            \"question_type\": \"MULTIPLE_CHOICE\",\n",
        "            \"choices\": [\"Red\", \"Blue\", \"Green\", \"Yellow\"],\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(\"Responses:\", responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m1dywxpIAvu"
      },
      "source": [
        "Now, let's define the interface of this function in JSON format, so our Assistant can call it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naHd0rBeIAvu"
      },
      "outputs": [],
      "source": [
        "function_json = {\n",
        "    \"name\": \"display_quiz\",\n",
        "    \"description\": \"Displays a quiz to the student, and returns the student's response. A single quiz can have multiple questions.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"title\": {\"type\": \"string\"},\n",
        "            \"questions\": {\n",
        "                \"type\": \"array\",\n",
        "                \"description\": \"An array of questions, each with a title and potentially options (if multiple choice).\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"question_text\": {\"type\": \"string\"},\n",
        "                        \"question_type\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"enum\": [\"MULTIPLE_CHOICE\", \"FREE_RESPONSE\"],\n",
        "                        },\n",
        "                        \"choices\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                    },\n",
        "                    \"required\": [\"question_text\"],\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"title\", \"questions\"],\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K9SIGvjIAvu"
      },
      "source": [
        "Once again, let's update our Assistant either through the Dashboard or the API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf6gRXkgIAvv"
      },
      "source": [
        "![Enabling custom function](../images/assistants_overview_enable_function.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvWzYYboIAvv"
      },
      "source": [
        "> **Note**\n",
        "> Pasting the function JSON into the Dashboard was a bit finicky due to indentation, etc. I just asked ChatGPT to format my function the same as one of the examples on the Dashboard :).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2KCgdIcIAvv",
        "outputId": "eb4c7f75-4c83-4f8f-91f5-509965802f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'created_at': 1710933670,\n",
              " 'description': None,\n",
              " 'file_ids': ['file-u5H0QWd95vzYHIEmJfR1Vn3E'],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'name': 'Math Tutor',\n",
              " 'object': 'assistant',\n",
              " 'tools': [{'type': 'code_interpreter'},\n",
              "  {'type': 'retrieval'},\n",
              "  {'function': {'name': 'display_quiz',\n",
              "    'description': \"Displays a quiz to the student, and returns the student's response. A single quiz can have multiple questions.\",\n",
              "    'parameters': {'type': 'object',\n",
              "     'properties': {'title': {'type': 'string'},\n",
              "      'questions': {'type': 'array',\n",
              "       'description': 'An array of questions, each with a title and potentially options (if multiple choice).',\n",
              "       'items': {'type': 'object',\n",
              "        'properties': {'question_text': {'type': 'string'},\n",
              "         'question_type': {'type': 'string',\n",
              "          'enum': ['MULTIPLE_CHOICE', 'FREE_RESPONSE']},\n",
              "         'choices': {'type': 'array', 'items': {'type': 'string'}}},\n",
              "        'required': ['question_text']}}},\n",
              "     'required': ['title', 'questions']}},\n",
              "   'type': 'function'}]}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "assistant = client.beta.assistants.update(\n",
        "    MATH_ASSISTANT_ID,\n",
        "    tools=[\n",
        "        {\"type\": \"code_interpreter\"},\n",
        "        {\"type\": \"retrieval\"},\n",
        "        {\"type\": \"function\", \"function\": function_json},\n",
        "    ],\n",
        ")\n",
        "show_json(assistant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J_1anYwIAvv"
      },
      "source": [
        "And now, we ask for a quiz.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlZayvBNIAvv",
        "outputId": "4ab01e32-5749-4174-e859-0d18fffd2809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'requires_action'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "thread, run = create_thread_and_run(\n",
        "    \"Make a quiz with 2 questions: One open ended, one multiple choice. Then, give me feedback for the responses.\"\n",
        ")\n",
        "run = wait_on_run(run, thread)\n",
        "run.status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKKW02A1IAvv"
      },
      "source": [
        "Now, however, when we check the Run's `status` we see `requires_action`! Let's take a closer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fkaWDkaIAvv",
        "outputId": "674576a8-69e2-4d2f-a027-632959647902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'run_6vZeKXfIH2w7ZWFzykFaQ6Rl',\n",
              " 'assistant_id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': None,\n",
              " 'created_at': 1710934626,\n",
              " 'expires_at': 1710935226,\n",
              " 'failed_at': None,\n",
              " 'file_ids': ['file-u5H0QWd95vzYHIEmJfR1Vn3E'],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'last_error': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': {'submit_tool_outputs': {'tool_calls': [{'id': 'call_PTIQgLYGFNrjtCyWJAqsgX75',\n",
              "     'function': {'arguments': '{\"title\":\"Math Quiz\",\"questions\":[{\"question_text\":\"What is the result of 5 multiplied by 7?\",\"question_type\":\"FREE_RESPONSE\"},{\"question_text\":\"Which of the following is a prime number?\",\"question_type\":\"MULTIPLE_CHOICE\",\"choices\":[\"9\",\"11\",\"14\",\"20\"]}]}',\n",
              "      'name': 'display_quiz'},\n",
              "     'type': 'function'}]},\n",
              "  'type': 'submit_tool_outputs'},\n",
              " 'started_at': 1710934642,\n",
              " 'status': 'requires_action',\n",
              " 'thread_id': 'thread_XdcqyZOU5RrLlfjfCKpJDicM',\n",
              " 'tools': [{'type': 'code_interpreter'},\n",
              "  {'type': 'retrieval'},\n",
              "  {'function': {'name': 'display_quiz',\n",
              "    'description': \"Displays a quiz to the student, and returns the student's response. A single quiz can have multiple questions.\",\n",
              "    'parameters': {'type': 'object',\n",
              "     'properties': {'title': {'type': 'string'},\n",
              "      'questions': {'type': 'array',\n",
              "       'description': 'An array of questions, each with a title and potentially options (if multiple choice).',\n",
              "       'items': {'type': 'object',\n",
              "        'properties': {'question_text': {'type': 'string'},\n",
              "         'question_type': {'type': 'string',\n",
              "          'enum': ['MULTIPLE_CHOICE', 'FREE_RESPONSE']},\n",
              "         'choices': {'type': 'array', 'items': {'type': 'string'}}},\n",
              "        'required': ['question_text']}}},\n",
              "     'required': ['title', 'questions']}},\n",
              "   'type': 'function'}],\n",
              " 'usage': None}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_json(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shFFezVXIAvv"
      },
      "source": [
        "The `required_action` field indicates a Tool is waiting for us to run it and submit its output back to the Assistant. Specifically, the `display_quiz` function! Let's start by parsing the `name` and `arguments`.\n",
        "\n",
        "> **Note**\n",
        "> While in this case we know there is only one Tool call, in practice the Assistant may choose to call multiple tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzCnVaP1IAvv",
        "outputId": "50c87644-2860-4234-b84e-763ffe3c4499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function Name: display_quiz\n",
            "Function Arguments:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Math Quiz',\n",
              " 'questions': [{'question_text': 'What is the result of 5 multiplied by 7?',\n",
              "   'question_type': 'FREE_RESPONSE'},\n",
              "  {'question_text': 'Which of the following is a prime number?',\n",
              "   'question_type': 'MULTIPLE_CHOICE',\n",
              "   'choices': ['9', '11', '14', '20']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Extract single tool call\n",
        "tool_call = run.required_action.submit_tool_outputs.tool_calls[0]\n",
        "name = tool_call.function.name\n",
        "arguments = json.loads(tool_call.function.arguments)\n",
        "\n",
        "print(\"Function Name:\", name)\n",
        "print(\"Function Arguments:\")\n",
        "arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UANzWQcMIAvv"
      },
      "source": [
        "Now let's actually call our `display_quiz` function with the arguments provided by the Assistant:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtmwvtUhIAvv",
        "outputId": "df0aedc5-cb3a-4291-9c44-b33124993ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quiz: Math Quiz\n",
            "\n",
            "What is the result of 5 multiplied by 7?\n",
            "\n",
            "Which of the following is a prime number?\n",
            "0. 9\n",
            "1. 11\n",
            "2. 14\n",
            "3. 20\n",
            "\n",
            "Responses: [\"I don't know.\", 'a']\n"
          ]
        }
      ],
      "source": [
        "responses = display_quiz(arguments[\"title\"], arguments[\"questions\"])\n",
        "print(\"Responses:\", responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seT2GYFhIAvv"
      },
      "source": [
        "Great! (Remember these responses are the one's we mocked earlier. In reality, we'd be getting input from the back from this function call.)\n",
        "\n",
        "Now that we have our responses, let's submit them back to the Assistant. We'll need the `tool_call` ID, found in the `tool_call` we parsed out earlier. We'll also need to encode our `list`of responses into a `str`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL-YveCrIAvw",
        "outputId": "b682c0ef-d4e4-46b5-8fff-7e0cfba4d30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'run_6vZeKXfIH2w7ZWFzykFaQ6Rl',\n",
              " 'assistant_id': 'asst_wIiQdreLlLNzoud1MxL2iheM',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': None,\n",
              " 'created_at': 1710934626,\n",
              " 'expires_at': 1710935226,\n",
              " 'failed_at': None,\n",
              " 'file_ids': ['file-u5H0QWd95vzYHIEmJfR1Vn3E'],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'last_error': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': None,\n",
              " 'started_at': 1710934642,\n",
              " 'status': 'queued',\n",
              " 'thread_id': 'thread_XdcqyZOU5RrLlfjfCKpJDicM',\n",
              " 'tools': [{'type': 'code_interpreter'},\n",
              "  {'type': 'retrieval'},\n",
              "  {'function': {'name': 'display_quiz',\n",
              "    'description': \"Displays a quiz to the student, and returns the student's response. A single quiz can have multiple questions.\",\n",
              "    'parameters': {'type': 'object',\n",
              "     'properties': {'title': {'type': 'string'},\n",
              "      'questions': {'type': 'array',\n",
              "       'description': 'An array of questions, each with a title and potentially options (if multiple choice).',\n",
              "       'items': {'type': 'object',\n",
              "        'properties': {'question_text': {'type': 'string'},\n",
              "         'question_type': {'type': 'string',\n",
              "          'enum': ['MULTIPLE_CHOICE', 'FREE_RESPONSE']},\n",
              "         'choices': {'type': 'array', 'items': {'type': 'string'}}},\n",
              "        'required': ['question_text']}}},\n",
              "     'required': ['title', 'questions']}},\n",
              "   'type': 'function'}],\n",
              " 'usage': None}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = client.beta.threads.runs.submit_tool_outputs(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id,\n",
        "    tool_outputs=[\n",
        "        {\n",
        "            \"tool_call_id\": tool_call.id,\n",
        "            \"output\": json.dumps(responses),\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "show_json(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do3VNnUjIAvw"
      },
      "source": [
        "We can now wait for the Run to complete once again, and check our Thread!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eOquJ__IAvw",
        "outputId": "18ed015c-af96-4b11-9b15-30e0f162fbb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Messages\n",
            "user: Make a quiz with 2 questions: One open ended, one multiple choice. Then, give me feedback for the responses.\n",
            "assistant: For the question \"What is the result of 5 multiplied by 7?\", your response was \"I don't know.\" The correct answer is 35.\n",
            "\n",
            "For the question \"Which of the following is a prime number?\", your response was \"a.\" The correct answer is \"11,\" which is a prime number.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run = wait_on_run(run, thread)\n",
        "pretty_print(get_response(thread))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLpjrfJbIAvw"
      },
      "source": [
        "Woohoo 🎉\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thi Đấu Assistant vs GPT-3.5"
      ],
      "metadata": {
        "id": "FnaY8FsJq2ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assistant"
      ],
      "metadata": {
        "id": "8Nh91Kvkx3i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Assistant\n",
        "assistant = client.beta.assistants.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    instructions=\"You are a math tutor.\",\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    file_ids=None\n",
        ")\n",
        "show_json(assistant)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "rHLQb8f9q1vt",
        "outputId": "bf5172fb-6733-4838-a672-36cf40b7b31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'asst_r7UpyWvElzUs3AzUmrvXUwsg',\n",
              " 'created_at': 1710936429,\n",
              " 'description': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a math tutor.',\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'name': None,\n",
              " 'object': 'assistant',\n",
              " 'tools': [{'type': 'code_interpreter'}]}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thread, run = create_thread_and_run(\"\"\"The test has 5 questions. Answer the questions with code:\n",
        "1. From a pack of 52 cards, a card is drawn at random. What is the probability of getting a queen?\n",
        "2. Throw 2 dices simultaneously. What is the probability that the summation of the numbers is multiply of 4?\n",
        "3. 10 coins are tossed, find the probability that two heads are obtained.\n",
        "4. If P(A)=0.4, P(B)=0.6 and P(A ∪ B)=0.8. What is the value of P(A∩B')=?\n",
        "5. Throw a dice 3 times. What's the probability that we have three 6?\"\"\", id=assistant.id)\n",
        "run = wait_on_run(run, thread)\n",
        "run.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fyh6vP3urcAw",
        "outputId": "3fe568fd-8a43-4322-e9e2-afc550d240d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'completed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_json(run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "VxdBi-u6sMVI",
        "outputId": "54c702f2-e8f7-4939-abbb-b028b36de21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'run_58jEVTHf8lNkMD9v8v03NKRz',\n",
              " 'assistant_id': 'asst_r7UpyWvElzUs3AzUmrvXUwsg',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': 1710936486,\n",
              " 'created_at': 1710936471,\n",
              " 'expires_at': None,\n",
              " 'failed_at': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a math tutor.',\n",
              " 'last_error': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': None,\n",
              " 'started_at': 1710936471,\n",
              " 'status': 'completed',\n",
              " 'thread_id': 'thread_3sCDSwKeHTTz53OcwuShCaMT',\n",
              " 'tools': [{'type': 'code_interpreter'}],\n",
              " 'usage': {'completion_tokens': 749,\n",
              "  'prompt_tokens': 1616,\n",
              "  'total_tokens': 2365}}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = wait_on_run(run, thread)\n",
        "pretty_print(get_response(thread))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9x7h4sHsSwx",
        "outputId": "251546d1-4935-4006-bc98-459b84082310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Messages\n",
            "user: The test has 5 questions. Answer the questions with code:\n",
            "1. From a pack of 52 cards, a card is drawn at random. What is the probability of getting a queen?\n",
            "2. Throw 2 dices simultaneously. What is the probability that the summation of the numbers is multiply of 4?\n",
            "3. 10 coins are tossed, find the probability that two heads are obtained.\n",
            "4. If P(A)=0.4, P(B)=0.6 and P(A ∪ B)=0.8. What is the value of P(A∩B')=?\n",
            "5. Throw a dice 3 times. What's the probability that we have three 6?\n",
            "assistant: Let's solve each of these questions one by one:\n",
            "\n",
            "1. The probability of getting a queen from a pack of 52 cards is given by the number of queens divided by the total number of cards.\n",
            "\\[P(\\text{Queen}) = \\frac{\\text{Number of Queens}}{\\text{Total Number of Cards}}\\]\n",
            "\n",
            "2. To find the probability that the sum of two dice is a multiple of 4, we need to calculate the total number of favorable outcomes and divide it by the total number of outcomes when throwing two dice.\n",
            "\n",
            "3. The probability of obtaining exactly two heads when tossing 10 coins follows a binomial distribution. We can calculate this probability using the binomial probability formula.\n",
            "\n",
            "4. We can use the formula for the probability of the union of two events to find \\(P(A \\cap B')\\).\n",
            "\n",
            "5. When throwing a dice 3 times, the probability of getting a specific number (in this case 6) on all three throws is calculated by multiplying the individual probabilities of getting a 6 on each throw.\n",
            "\n",
            "Let's calculate the probabilities for each of these questions.\n",
            "assistant: Here are the probabilities for each of the questions:\n",
            "\n",
            "1. The probability of getting a queen from a pack of 52 cards is approximately 0.077 or 7.69%.\n",
            "2. The probability that the sum of two dice is a multiple of 4 is approximately 0.278 or 27.78%.\n",
            "3. The probability of obtaining exactly two heads when tossing 10 coins is approximately 0.044 or 4.39%.\n",
            "4. The value of \\(P(A \\cap B')\\) is approximately 0.200 or 20%.\n",
            "5. The probability of getting three 6's when throwing a dice 3 times is approximately 0.005 or 0.46%.\n",
            "\n",
            "If you have any more questions or need further clarification, feel free to ask!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_steps = client.beta.threads.runs.steps.list(\n",
        "    thread_id=thread.id, run_id=run.id, order=\"asc\"\n",
        ")"
      ],
      "metadata": {
        "id": "oBCeAxaHskSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_steps.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHMZ4ClNsvJg",
        "outputId": "114afa18-6b98-414b-dff5-6e885744effe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[RunStep(id='step_bKjQbI7xs2B6zpfu8uODNer8', assistant_id='asst_r7UpyWvElzUs3AzUmrvXUwsg', cancelled_at=None, completed_at=1710936476, created_at=1710936471, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_58jEVTHf8lNkMD9v8v03NKRz', status='completed', step_details=MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_idA7Hx4IIz4hNlql5O6anctH'), type='message_creation'), thread_id='thread_3sCDSwKeHTTz53OcwuShCaMT', type='message_creation', usage=Usage(completion_tokens=227, prompt_tokens=248, total_tokens=475), expires_at=None),\n",
              " RunStep(id='step_oaBeIV3SCkbuB3qATVEZy3Fo', assistant_id='asst_r7UpyWvElzUs3AzUmrvXUwsg', cancelled_at=None, completed_at=1710936484, created_at=1710936476, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_58jEVTHf8lNkMD9v8v03NKRz', status='completed', step_details=ToolCallsStepDetails(tool_calls=[CodeInterpreterToolCall(id='call_IZYGa3FsRmKW6c9qPZHuCHrY', code_interpreter=CodeInterpreter(input=\"# 1. Probability of getting a queen from a pack of 52 cards\\r\\ntotal_cards = 52\\r\\nqueens = 4\\r\\nprob_queen = queens / total_cards\\r\\n\\r\\n# 2. Probability that the sum of two dice is a multiple of 4\\r\\ntotal_outcomes = 6 * 6  # total outcomes when throwing two dice\\r\\n# favorable outcomes: (1,3), (1,3), (3,1), (3,3), (2,2), (4,4), (5,3), (3,5), (4,6), (6,4)\\r\\nfavorable_outcomes = 10\\r\\nprob_sum_multiple_4 = favorable_outcomes / total_outcomes\\r\\n\\r\\n# 3. Probability of obtaining exactly two heads when tossing 10 coins\\r\\nfrom math import comb\\r\\ntotal_coin_tosses = 10\\r\\ndesired_heads = 2\\r\\nprob_two_heads = comb(total_coin_tosses, desired_heads) * (0.5 ** desired_heads) * (0.5 ** (total_coin_tosses - desired_heads))\\r\\n\\r\\n# 4. Probability of the intersection of A and the complement of B\\r\\nprob_A = 0.4\\r\\nprob_B = 0.6\\r\\nprob_A_union_B = 0.8\\r\\nprob_A_intersect_B_complement = prob_A - (prob_A_union_B - prob_B)\\r\\n\\r\\n# 5. Probability of getting three 6's when throwing a dice 3 times\\r\\nprob_six = 1/6\\r\\nprob_three_six = prob_six ** 3\\r\\n\\r\\nprob_queen, prob_sum_multiple_4, prob_two_heads, prob_A_intersect_B_complement, prob_three_six\", outputs=[CodeInterpreterOutputLogs(logs='(0.07692307692307693,\\n 0.2777777777777778,\\n 0.0439453125,\\n 0.19999999999999996,\\n 0.0046296296296296285)', type='logs')]), type='code_interpreter')], type='tool_calls'), thread_id='thread_3sCDSwKeHTTz53OcwuShCaMT', type='tool_calls', usage=Usage(completion_tokens=357, prompt_tokens=477, total_tokens=834), expires_at=None),\n",
              " RunStep(id='step_DOFiBkhc05Akkcnjyu9OzDfw', assistant_id='asst_r7UpyWvElzUs3AzUmrvXUwsg', cancelled_at=None, completed_at=1710936486, created_at=1710936484, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_58jEVTHf8lNkMD9v8v03NKRz', status='completed', step_details=MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_Lnb4NusKZnAmTtZiBdcw4azV'), type='message_creation'), thread_id='thread_3sCDSwKeHTTz53OcwuShCaMT', type='message_creation', usage=Usage(completion_tokens=165, prompt_tokens=891, total_tokens=1056), expires_at=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_response = get_response(thread).data[-1].content[0].text.value\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNPTrDSkvPN4",
        "outputId": "ca35cb3d-5617-4c5d-f5ae-725cf50a5487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the probabilities for each of the questions:\n",
            "\n",
            "1. The probability of getting a queen from a pack of 52 cards is approximately 0.077 or 7.69%.\n",
            "2. The probability that the sum of two dice is a multiple of 4 is approximately 0.278 or 27.78%.\n",
            "3. The probability of obtaining exactly two heads when tossing 10 coins is approximately 0.044 or 4.39%.\n",
            "4. The value of \\(P(A \\cap B')\\) is approximately 0.200 or 20%.\n",
            "5. The probability of getting three 6's when throwing a dice 3 times is approximately 0.005 or 0.46%.\n",
            "\n",
            "If you have any more questions or need further clarification, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze0T3v8XxBhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-3.5"
      ],
      "metadata": {
        "id": "X7cuLja0x8jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "gpt3_5_response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': \"\"\"The test has 5 questions. Answer each question very short, in less than a sentence:\n",
        "1. From a pack of 52 cards, a card is drawn at random. What is the probability of getting a queen?\n",
        "2. Throw 2 dices simultaneously. What is the probability that the summation of the numbers is multiply of 4?\n",
        "3. 10 coins are tossed, find the probability that two heads are obtained.\n",
        "4. If P(A)=0.4, P(B)=0.6 and P(A ∪ B)=0.8. What is the value of P(A∩B')=?\n",
        "5. Throw a dice 3 times. What's the probability that we have three 6?\"\"\"}]\n",
        ")"
      ],
      "metadata": {
        "id": "j1cZki28yAnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_5_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DSRZxzorySOY",
        "outputId": "84917ddb-9b21-416d-a7e2-bec8d247d1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. 4/52 or 1/13\\n2. 4/36 or 1/9\\n3. 9/1024 or 0.0088\\n4. 0.2 \\n5. 1/216'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAH6PrOgyqZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Gw2FEBUWIAvn",
        "kQIsCHyOIAvp",
        "m2z8wtXLIAvq"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}