{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e16f8b823b8e400c8b2c8bfc2220c7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d551bf2f93d41cfbbb5bad5f19b8656",
              "IPY_MODEL_41cf03e2baf2469897c89b917d897f91",
              "IPY_MODEL_c998a51cfcd847079959a57b64a9fa9c"
            ],
            "layout": "IPY_MODEL_74c43504e4724ffa9b5fc48669428b86"
          }
        },
        "6d551bf2f93d41cfbbb5bad5f19b8656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85768aa38b543f49a64a2bd5bc726bf",
            "placeholder": "​",
            "style": "IPY_MODEL_4b25b580fe3a4bb684fecf0ee0de6701",
            "value": "model.safetensors: 100%"
          }
        },
        "41cf03e2baf2469897c89b917d897f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44118330b4984efdabdd3e67daf59502",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90ba50a5e80e4b6eb8198767dce91af6",
            "value": 548105171
          }
        },
        "c998a51cfcd847079959a57b64a9fa9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca507aacbf2a417694c14aa7ef0fb675",
            "placeholder": "​",
            "style": "IPY_MODEL_abe80f65727e43f28aeddaf331b78e17",
            "value": " 548M/548M [00:06&lt;00:00, 166MB/s]"
          }
        },
        "74c43504e4724ffa9b5fc48669428b86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85768aa38b543f49a64a2bd5bc726bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b25b580fe3a4bb684fecf0ee0de6701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44118330b4984efdabdd3e67daf59502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ba50a5e80e4b6eb8198767dce91af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca507aacbf2a417694c14aa7ef0fb675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe80f65727e43f28aeddaf331b78e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucaokylong/LLM_learning/blob/main/Fine_tune_gpt_multitask_autoregressive_and_classification_with_dualLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vtBylvhXw0R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 16X2D5KrBsP0nTojfrYu9zrBMVxcC7JsX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_U5VtSWoljs",
        "outputId": "4910712c-03af-46a9-e578-4cafa793ecb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16X2D5KrBsP0nTojfrYu9zrBMVxcC7JsX\n",
            "To: /content/train_nor_811.xlsx\n",
            "\r  0% 0.00/259k [00:00<?, ?B/s]\r100% 259k/259k [00:00<00:00, 87.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1DOkLy8-aO1_KlwvkuxVIA9jvvGycDRzp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPYDzmL7orOs",
        "outputId": "a4e292d8-38f5-4c4f-f012-a67924866ce4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DOkLy8-aO1_KlwvkuxVIA9jvvGycDRzp\n",
            "To: /content/test_nor_811.xlsx\n",
            "\r  0% 0.00/37.6k [00:00<?, ?B/s]\r100% 37.6k/37.6k [00:00<00:00, 62.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1OOvkJmkQE5O3nQDrHnbu2tjuDkMM1PAU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owm_IBBdo7Hc",
        "outputId": "4c7a687d-55bb-40aa-8963-3d809a3694d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OOvkJmkQE5O3nQDrHnbu2tjuDkMM1PAU\n",
            "To: /content/valid_nor_811.xlsx\n",
            "\r  0% 0.00/38.0k [00:00<?, ?B/s]\r100% 38.0k/38.0k [00:00<00:00, 63.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def get_data(path):\n",
        "    df = pd.read_excel(path, sheet_name=None)['Sheet1']\n",
        "    df.columns = ['index', 'Emotion', 'Sentence']\n",
        "    df.drop(columns=['index'], inplace=True)\n",
        "    return df\n",
        "\n",
        "train_df = get_data('./train_nor_811.xlsx')\n",
        "valid_df = get_data('./valid_nor_811.xlsx')\n",
        "test_df = get_data('./test_nor_811.xlsx')\n"
      ],
      "metadata": {
        "id": "ZFrqQrNSpAkI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XE6SP47TpFIm",
        "outputId": "098191c7-642a-44a6-c28c-3aaa32053cf9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Emotion                                           Sentence\n",
              "0      Other              cho mình xin bài nhạc tên là gì với ạ\n",
              "1    Disgust  cho đáng đời con quỷ . về nhà lôi con nhà mày ...\n",
              "2    Disgust  lo học đi . yêu đương lol gì hay lại thích học...\n",
              "3  Enjoyment    uớc gì sau này về già vẫn có thể như cụ này :))\n",
              "4  Enjoyment  mỗi lần có video của con là cứ coi đi coi lại ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b3db200-cb72-4649-a5e2-045735743c7c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Other</td>\n",
              "      <td>cho mình xin bài nhạc tên là gì với ạ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Disgust</td>\n",
              "      <td>cho đáng đời con quỷ . về nhà lôi con nhà mày ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Disgust</td>\n",
              "      <td>lo học đi . yêu đương lol gì hay lại thích học...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Enjoyment</td>\n",
              "      <td>uớc gì sau này về già vẫn có thể như cụ này :))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Enjoyment</td>\n",
              "      <td>mỗi lần có video của con là cứ coi đi coi lại ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b3db200-cb72-4649-a5e2-045735743c7c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9b3db200-cb72-4649-a5e2-045735743c7c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9b3db200-cb72-4649-a5e2-045735743c7c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-07695a3e-48ae-42b0-8804-31c1f6c1828c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07695a3e-48ae-42b0-8804-31c1f6c1828c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-07695a3e-48ae-42b0-8804-31c1f6c1828c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 5548,\n  \"fields\": [\n    {\n      \"column\": \"Emotion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Other\",\n          \"Disgust\",\n          \"Sadness\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5544,\n        \"samples\": [\n          \"quan tr\\u1ecdng v\\u1eabn l\\u00e0 c\\u00e1i ph\\u1ea3i \\u0111\\u1eb9p trai . r\\u1ed3i m\\u1edbi sau \\u0111\\u00f3 n\\u1eefa ... l\\u1ea1 g\\u00ec c\\u00e1c b\\u00e0\",\n          \"tao \\u0111\\u00e9o gh\\u00ea con r\\u1eafn tao gh\\u00ea m\\u1ea5y c\\u00e1i tr\\u1ed3i l\\u00ean k\\u00eca :)))\",\n          \"c\\u00f3 1 \\u0111\\u1ee9a h\\u1ee3p t\\u00e1c v\\u1eady l\\u00e0 c\\u0169g qu\\u00fd ho\\u00e1 qu\\u00e1 r\\u1ed3i\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "aqqk-knvpXtF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=120):\n",
        "        \"\"\"\n",
        "        Dataset for dual-loss training (language modeling and classification).\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            {\n",
        "                'text': Original text,\n",
        "                'input_ids': Tokenized input IDs for the model,\n",
        "                'attention_masks': Attention masks for padding,\n",
        "                'lm_labels': Shifted input IDs for language modeling loss,\n",
        "                'classification_labels': Target labels for classification loss\n",
        "            }\n",
        "        \"\"\"\n",
        "        row = self.df.iloc[index]\n",
        "        text, classification_label = self.get_input_data(row)\n",
        "\n",
        "        # Tokenize the input for language modeling and classification\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].flatten()\n",
        "        # Shift labels for language modeling (next token prediction)\n",
        "        lm_labels = input_ids.clone()\n",
        "        lm_labels[lm_labels == self.tokenizer.pad_token_id] = -100  # Ignore pad tokens in loss\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_masks': encoding['attention_mask'].flatten(),\n",
        "            'lm_labels': lm_labels,\n",
        "            'classification_labels': torch.tensor(classification_label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "    def labelencoder(self, text):\n",
        "        \"\"\"\n",
        "        Maps emotion labels to integer indices.\n",
        "        \"\"\"\n",
        "        label_map = {\n",
        "            'Enjoyment': 0,\n",
        "            'Disgust': 1,\n",
        "            'Sadness': 2,\n",
        "            'Anger': 3,\n",
        "            'Surprise': 4,\n",
        "            'Fear': 5,\n",
        "        }\n",
        "        return label_map.get(text, 6)  # Default to 6 for unknown labels\n",
        "\n",
        "    def get_input_data(self, row):\n",
        "        \"\"\"\n",
        "        Preprocesses the input text and retrieves the classification label.\n",
        "        \"\"\"\n",
        "        # Text preprocessing: remove special characters, lowercasing\n",
        "        text = row['Sentence']\n",
        "        text = ' '.join(text.lower().split())  # Simplified preprocessing\n",
        "        label = self.labelencoder(row['Emotion'])\n",
        "        return text, label"
      ],
      "metadata": {
        "id": "wwnARM5zpNA8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Initialize tokenizer and dataset\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is defined\n",
        "# Initialize datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "6WP5fKm-pSWL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentimentDataset(train_df, tokenizer, max_len=32)\n",
        "validation_dataset = SentimentDataset(valid_df, tokenizer, max_len=32)\n",
        "test_dataset = SentimentDataset(test_df, tokenizer, max_len=32)"
      ],
      "metadata": {
        "id": "udB-291upfcW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMG4tKsIqAS5",
        "outputId": "e1ed5940-6b28-4ecc-cd63-8f73b0770684"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'cho mình xin bài nhạc tên là gì với ạ',\n",
              " 'input_ids': tensor([ 6679,   285,   127,   105,    77,    71,  2124,   259,   275, 24247,\n",
              "            72,   299,    71,   157,   118,    94,    66,   256, 25792,    77,\n",
              "           300, 24247,   308,   127,   105,   410,   157,   119,   249,    72,\n",
              "         28053,   118]),\n",
              " 'attention_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'lm_labels': tensor([ 6679,   285,   127,   105,    77,    71,  2124,   259,   275, 24247,\n",
              "            72,   299,    71,   157,   118,    94,    66,   256, 25792,    77,\n",
              "           300, 24247,   308,   127,   105,   410,   157,   119,   249,    72,\n",
              "         28053,   118]),\n",
              " 'classification_labels': tensor(6)}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "HSSQ-3zZqEet"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n"
      ],
      "metadata": {
        "id": "xE0isP2wqute"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "4VOJeHv_q4eD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "vcjd3ZVgrEnY",
        "outputId": "7a1055d2-1492-4e68-d2b6-e48bed7bb254"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.0\n",
            "    Uninstalling transformers-4.47.0:\n",
            "      Successfully uninstalled transformers-4.47.0\n",
            "Successfully installed transformers-4.47.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "adf42e029565406db8b913cc3773d1c8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Model"
      ],
      "metadata": {
        "id": "RfJlRT2Zq55i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XWZRG69igiPe"
      },
      "outputs": [],
      "source": [
        "class TransformerWithDualLoss(nn.Module):\n",
        "    def __init__(self, transformer_model_name, num_classes):\n",
        "        \"\"\"\n",
        "        Transformer model with dual loss: language modeling loss and classification loss.\n",
        "        \"\"\"\n",
        "        super(TransformerWithDualLoss, self).__init__()\n",
        "        self.transformer = GPT2Model.from_pretrained(transformer_model_name)\n",
        "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_classes)\n",
        "        self.lm_head = nn.Linear(self.transformer.config.hidden_size, self.transformer.config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, lm_labels=None, classification_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass for both language modeling and classification tasks.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs (batch_size, seq_length).\n",
        "            attention_mask: Attention mask (batch_size, seq_length).\n",
        "            lm_labels: Labels for the language modeling task (batch_size, seq_length).\n",
        "            classification_labels: Labels for the classification task (batch_size).\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing losses and logits:\n",
        "                - lm_loss: Language modeling loss.\n",
        "                - classification_loss: Classification loss.\n",
        "                - lm_logits: Language modeling logits.\n",
        "                - classification_logits: Classification logits.\n",
        "        \"\"\"\n",
        "        # Forward pass through the transformer\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Language modeling logits and loss\n",
        "        lm_loss = None\n",
        "        if lm_labels is not None:\n",
        "            lm_logits = self.lm_head(last_hidden_state)  # Predict next token probabilities\n",
        "            lm_loss_fct = nn.CrossEntropyLoss()\n",
        "            # Shift logits and labels for language modeling\n",
        "            shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "            shift_labels = lm_labels[:, 1:].contiguous()\n",
        "            lm_loss = lm_loss_fct(shift_logits.view(-1, lm_logits.size(-1)), shift_labels.view(-1))\n",
        "        else:\n",
        "            lm_logits = None\n",
        "\n",
        "        # Classification logits and loss\n",
        "        classification_loss = None\n",
        "        pooled_output = last_hidden_state.mean(dim=1)  # Mean pooling for sentence-level representation\n",
        "        classification_logits = self.classifier(pooled_output)  # Predict class probabilities\n",
        "        if classification_labels is not None:\n",
        "            classification_loss_fct = nn.CrossEntropyLoss()\n",
        "            classification_loss = classification_loss_fct(classification_logits, classification_labels)\n",
        "\n",
        "        # Return both losses and logits\n",
        "        return {\n",
        "            \"lm_loss\": lm_loss,\n",
        "            \"classification_loss\": classification_loss,\n",
        "            \"lm_logits\": lm_logits,\n",
        "            \"classification_logits\": classification_logits\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerWithDualLoss(\n",
        "    transformer_model_name=\"gpt2\", num_classes=7\n",
        "    )"
      ],
      "metadata": {
        "id": "MwmsOLAyr0SB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e16f8b823b8e400c8b2c8bfc2220c7a9",
            "6d551bf2f93d41cfbbb5bad5f19b8656",
            "41cf03e2baf2469897c89b917d897f91",
            "c998a51cfcd847079959a57b64a9fa9c",
            "74c43504e4724ffa9b5fc48669428b86",
            "a85768aa38b543f49a64a2bd5bc726bf",
            "4b25b580fe3a4bb684fecf0ee0de6701",
            "44118330b4984efdabdd3e67daf59502",
            "90ba50a5e80e4b6eb8198767dce91af6",
            "ca507aacbf2a417694c14aa7ef0fb675",
            "abe80f65727e43f28aeddaf331b78e17"
          ]
        },
        "outputId": "9cb9b68b-2bb5-4090-f210-b13d3feb6850"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e16f8b823b8e400c8b2c8bfc2220c7a9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "metadata": {
        "id": "Tjd0HpUHsgIz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    epochs=3,\n",
        "    alpha=0.5,\n",
        "    clip_grad=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the TransformerWithDualLoss model with validation.\n",
        "\n",
        "    Args:\n",
        "        model: The model instance.\n",
        "        train_loader: DataLoader for the training dataset.\n",
        "        val_loader: DataLoader for the validation dataset.\n",
        "        optimizer: Optimizer instance.\n",
        "        device: Device to train on (CPU/GPU).\n",
        "        epochs: Number of training epochs.\n",
        "        alpha: Weight for combining losses (0.5 = equal weight for both losses).\n",
        "        clip_grad: Gradient clipping value.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_total_loss = 0.0\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Training Phase\")\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_masks'].to(device)\n",
        "            lm_labels = batch['lm_labels'].to(device)\n",
        "            classification_labels = batch['classification_labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                lm_labels=lm_labels,\n",
        "                classification_labels=classification_labels,\n",
        "            )\n",
        "\n",
        "            # Combine losses\n",
        "            lm_loss = outputs[\"lm_loss\"]\n",
        "            classification_loss = outputs[\"classification_loss\"]\n",
        "            if lm_loss is None:\n",
        "                total_batch_loss = classification_loss\n",
        "            elif classification_loss is None:\n",
        "                total_batch_loss = lm_loss\n",
        "            else:\n",
        "                total_batch_loss = alpha * lm_loss + (1 - alpha) * classification_loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            total_batch_loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), clip_grad)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update total loss\n",
        "            train_total_loss += total_batch_loss.item()\n",
        "\n",
        "            # Log batch loss\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(train_loader)} - Loss: {total_batch_loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = train_total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_total_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_masks'].to(device)\n",
        "                lm_labels = batch['lm_labels'].to(device)\n",
        "                classification_labels = batch['classification_labels'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    lm_labels=lm_labels,\n",
        "                    classification_labels=classification_labels,\n",
        "                )\n",
        "\n",
        "                # Combine losses\n",
        "                lm_loss = outputs[\"lm_loss\"]\n",
        "                classification_loss = outputs[\"classification_loss\"]\n",
        "                if lm_loss is None:\n",
        "                    total_batch_loss = classification_loss\n",
        "                elif classification_loss is None:\n",
        "                    total_batch_loss = lm_loss\n",
        "                else:\n",
        "                    total_batch_loss = alpha * lm_loss + (1 - alpha) * classification_loss\n",
        "\n",
        "                # Update total validation loss\n",
        "                val_total_loss += total_batch_loss.item()\n",
        "\n",
        "        avg_val_loss = val_total_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch + 1} - Average Validation Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "qoEdMcxJrqRs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "f24JDa3Nr-tV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Train the model\n",
        "train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    validation_loader, optimizer, device, epochs=3, alpha=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsdEhPYirspa",
        "outputId": "efbe4d05-d0a2-4e97-e465-c6f36114613c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Training Phase\n",
            "  Batch 1/174 - Loss: 11.4017\n",
            "  Batch 2/174 - Loss: 10.0754\n",
            "  Batch 3/174 - Loss: 8.5706\n",
            "  Batch 4/174 - Loss: 7.2906\n",
            "  Batch 5/174 - Loss: 6.8574\n",
            "  Batch 6/174 - Loss: 6.6988\n",
            "  Batch 7/174 - Loss: 6.4868\n",
            "  Batch 8/174 - Loss: 6.4314\n",
            "  Batch 9/174 - Loss: 6.2490\n",
            "  Batch 10/174 - Loss: 6.3019\n",
            "  Batch 11/174 - Loss: 6.2384\n",
            "  Batch 12/174 - Loss: 6.2138\n",
            "  Batch 13/174 - Loss: 6.1347\n",
            "  Batch 14/174 - Loss: 6.1212\n",
            "  Batch 15/174 - Loss: 6.1461\n",
            "  Batch 16/174 - Loss: 6.1262\n",
            "  Batch 17/174 - Loss: 6.0127\n",
            "  Batch 18/174 - Loss: 6.1062\n",
            "  Batch 19/174 - Loss: 5.9157\n",
            "  Batch 20/174 - Loss: 6.0139\n",
            "  Batch 21/174 - Loss: 5.9201\n",
            "  Batch 22/174 - Loss: 5.8319\n",
            "  Batch 23/174 - Loss: 5.9330\n",
            "  Batch 24/174 - Loss: 5.8356\n",
            "  Batch 25/174 - Loss: 5.7651\n",
            "  Batch 26/174 - Loss: 5.6375\n",
            "  Batch 27/174 - Loss: 5.5288\n",
            "  Batch 28/174 - Loss: 5.5433\n",
            "  Batch 29/174 - Loss: 5.4497\n",
            "  Batch 30/174 - Loss: 5.4031\n",
            "  Batch 31/174 - Loss: 5.3996\n",
            "  Batch 32/174 - Loss: 5.3769\n",
            "  Batch 33/174 - Loss: 5.1363\n",
            "  Batch 34/174 - Loss: 5.1580\n",
            "  Batch 35/174 - Loss: 5.0522\n",
            "  Batch 36/174 - Loss: 4.8895\n",
            "  Batch 37/174 - Loss: 4.9977\n",
            "  Batch 38/174 - Loss: 4.6841\n",
            "  Batch 39/174 - Loss: 4.6547\n",
            "  Batch 40/174 - Loss: 4.6263\n",
            "  Batch 41/174 - Loss: 4.5062\n",
            "  Batch 42/174 - Loss: 4.3193\n",
            "  Batch 43/174 - Loss: 4.5095\n",
            "  Batch 44/174 - Loss: 4.4720\n",
            "  Batch 45/174 - Loss: 4.3373\n",
            "  Batch 46/174 - Loss: 4.2607\n",
            "  Batch 47/174 - Loss: 4.1199\n",
            "  Batch 48/174 - Loss: 4.0501\n",
            "  Batch 49/174 - Loss: 4.0703\n",
            "  Batch 50/174 - Loss: 3.9436\n",
            "  Batch 51/174 - Loss: 3.9679\n",
            "  Batch 52/174 - Loss: 3.9308\n",
            "  Batch 53/174 - Loss: 3.8717\n",
            "  Batch 54/174 - Loss: 3.9230\n",
            "  Batch 55/174 - Loss: 3.8016\n",
            "  Batch 56/174 - Loss: 3.7798\n",
            "  Batch 57/174 - Loss: 3.6831\n",
            "  Batch 58/174 - Loss: 3.6421\n",
            "  Batch 59/174 - Loss: 3.5402\n",
            "  Batch 60/174 - Loss: 3.4989\n",
            "  Batch 61/174 - Loss: 3.4938\n",
            "  Batch 62/174 - Loss: 3.4304\n",
            "  Batch 63/174 - Loss: 3.3328\n",
            "  Batch 64/174 - Loss: 3.3225\n",
            "  Batch 65/174 - Loss: 3.2911\n",
            "  Batch 66/174 - Loss: 3.2300\n",
            "  Batch 67/174 - Loss: 3.2630\n",
            "  Batch 68/174 - Loss: 3.2059\n",
            "  Batch 69/174 - Loss: 3.3138\n",
            "  Batch 70/174 - Loss: 3.3952\n",
            "  Batch 71/174 - Loss: 3.3339\n",
            "  Batch 72/174 - Loss: 3.3174\n",
            "  Batch 73/174 - Loss: 3.0756\n",
            "  Batch 74/174 - Loss: 3.0094\n",
            "  Batch 75/174 - Loss: 3.0541\n",
            "  Batch 76/174 - Loss: 3.0384\n",
            "  Batch 77/174 - Loss: 2.8455\n",
            "  Batch 78/174 - Loss: 2.9338\n",
            "  Batch 79/174 - Loss: 3.1211\n",
            "  Batch 80/174 - Loss: 2.8700\n",
            "  Batch 81/174 - Loss: 2.9206\n",
            "  Batch 82/174 - Loss: 2.8919\n",
            "  Batch 83/174 - Loss: 2.9481\n",
            "  Batch 84/174 - Loss: 2.9378\n",
            "  Batch 85/174 - Loss: 2.8220\n",
            "  Batch 86/174 - Loss: 2.7890\n",
            "  Batch 87/174 - Loss: 2.9024\n",
            "  Batch 88/174 - Loss: 2.9430\n",
            "  Batch 89/174 - Loss: 2.6818\n",
            "  Batch 90/174 - Loss: 2.7674\n",
            "  Batch 91/174 - Loss: 2.7899\n",
            "  Batch 92/174 - Loss: 2.6630\n",
            "  Batch 93/174 - Loss: 2.6205\n",
            "  Batch 94/174 - Loss: 2.8735\n",
            "  Batch 95/174 - Loss: 2.8303\n",
            "  Batch 96/174 - Loss: 2.7113\n",
            "  Batch 97/174 - Loss: 2.7394\n",
            "  Batch 98/174 - Loss: 2.8777\n",
            "  Batch 99/174 - Loss: 2.7357\n",
            "  Batch 100/174 - Loss: 2.8123\n",
            "  Batch 101/174 - Loss: 2.7346\n",
            "  Batch 102/174 - Loss: 2.7319\n",
            "  Batch 103/174 - Loss: 2.6101\n",
            "  Batch 104/174 - Loss: 2.8269\n",
            "  Batch 105/174 - Loss: 2.6475\n",
            "  Batch 106/174 - Loss: 2.6477\n",
            "  Batch 107/174 - Loss: 2.6228\n",
            "  Batch 108/174 - Loss: 2.7039\n",
            "  Batch 109/174 - Loss: 2.7195\n",
            "  Batch 110/174 - Loss: 2.7443\n",
            "  Batch 111/174 - Loss: 2.6373\n",
            "  Batch 112/174 - Loss: 2.6530\n",
            "  Batch 113/174 - Loss: 2.5305\n",
            "  Batch 114/174 - Loss: 2.6083\n",
            "  Batch 115/174 - Loss: 2.5419\n",
            "  Batch 116/174 - Loss: 2.6300\n",
            "  Batch 117/174 - Loss: 2.6323\n",
            "  Batch 118/174 - Loss: 2.5942\n",
            "  Batch 119/174 - Loss: 2.6457\n",
            "  Batch 120/174 - Loss: 2.5092\n",
            "  Batch 121/174 - Loss: 2.5651\n",
            "  Batch 122/174 - Loss: 2.6139\n",
            "  Batch 123/174 - Loss: 2.6399\n",
            "  Batch 124/174 - Loss: 2.4859\n",
            "  Batch 125/174 - Loss: 2.4508\n",
            "  Batch 126/174 - Loss: 2.5306\n",
            "  Batch 127/174 - Loss: 2.6517\n",
            "  Batch 128/174 - Loss: 2.5335\n",
            "  Batch 129/174 - Loss: 2.5556\n",
            "  Batch 130/174 - Loss: 2.5036\n",
            "  Batch 131/174 - Loss: 2.5737\n",
            "  Batch 132/174 - Loss: 2.4551\n",
            "  Batch 133/174 - Loss: 2.5226\n",
            "  Batch 134/174 - Loss: 2.5074\n",
            "  Batch 135/174 - Loss: 2.4427\n",
            "  Batch 136/174 - Loss: 2.5448\n",
            "  Batch 137/174 - Loss: 2.4928\n",
            "  Batch 138/174 - Loss: 2.5488\n",
            "  Batch 139/174 - Loss: 2.3342\n",
            "  Batch 140/174 - Loss: 2.5386\n",
            "  Batch 141/174 - Loss: 2.4318\n",
            "  Batch 142/174 - Loss: 2.4193\n",
            "  Batch 143/174 - Loss: 2.4121\n",
            "  Batch 144/174 - Loss: 2.4300\n",
            "  Batch 145/174 - Loss: 2.3239\n",
            "  Batch 146/174 - Loss: 2.3701\n",
            "  Batch 147/174 - Loss: 2.4374\n",
            "  Batch 148/174 - Loss: 2.4462\n",
            "  Batch 149/174 - Loss: 2.4107\n",
            "  Batch 150/174 - Loss: 2.5003\n",
            "  Batch 151/174 - Loss: 2.3707\n",
            "  Batch 152/174 - Loss: 2.3299\n",
            "  Batch 153/174 - Loss: 2.3500\n",
            "  Batch 154/174 - Loss: 2.5603\n",
            "  Batch 155/174 - Loss: 2.4254\n",
            "  Batch 156/174 - Loss: 2.5414\n",
            "  Batch 157/174 - Loss: 2.5084\n",
            "  Batch 158/174 - Loss: 2.4036\n",
            "  Batch 159/174 - Loss: 2.4230\n",
            "  Batch 160/174 - Loss: 2.2977\n",
            "  Batch 161/174 - Loss: 2.4455\n",
            "  Batch 162/174 - Loss: 2.3238\n",
            "  Batch 163/174 - Loss: 2.3631\n",
            "  Batch 164/174 - Loss: 2.4277\n",
            "  Batch 165/174 - Loss: 2.4782\n",
            "  Batch 166/174 - Loss: 2.3436\n",
            "  Batch 167/174 - Loss: 2.3772\n",
            "  Batch 168/174 - Loss: 2.3298\n",
            "  Batch 169/174 - Loss: 2.3293\n",
            "  Batch 170/174 - Loss: 2.2855\n",
            "  Batch 171/174 - Loss: 2.3981\n",
            "  Batch 172/174 - Loss: 2.5094\n",
            "  Batch 173/174 - Loss: 2.4014\n",
            "  Batch 174/174 - Loss: 2.2439\n",
            "Epoch 1 - Average Training Loss: 3.6189\n",
            "Epoch 1 - Average Validation Loss: 2.2918\n",
            "Epoch 2/3 - Training Phase\n",
            "  Batch 1/174 - Loss: 2.3792\n",
            "  Batch 2/174 - Loss: 2.3227\n",
            "  Batch 3/174 - Loss: 2.2997\n",
            "  Batch 4/174 - Loss: 2.3872\n",
            "  Batch 5/174 - Loss: 2.3615\n",
            "  Batch 6/174 - Loss: 2.5508\n",
            "  Batch 7/174 - Loss: 2.3434\n",
            "  Batch 8/174 - Loss: 2.3536\n",
            "  Batch 9/174 - Loss: 2.3264\n",
            "  Batch 10/174 - Loss: 2.3382\n",
            "  Batch 11/174 - Loss: 2.2894\n",
            "  Batch 12/174 - Loss: 2.1893\n",
            "  Batch 13/174 - Loss: 2.2241\n",
            "  Batch 14/174 - Loss: 2.4677\n",
            "  Batch 15/174 - Loss: 2.3837\n",
            "  Batch 16/174 - Loss: 2.3320\n",
            "  Batch 17/174 - Loss: 2.2269\n",
            "  Batch 18/174 - Loss: 2.3192\n",
            "  Batch 19/174 - Loss: 2.2795\n",
            "  Batch 20/174 - Loss: 2.2281\n",
            "  Batch 21/174 - Loss: 2.2838\n",
            "  Batch 22/174 - Loss: 2.4092\n",
            "  Batch 23/174 - Loss: 2.4055\n",
            "  Batch 24/174 - Loss: 2.3199\n",
            "  Batch 25/174 - Loss: 2.3002\n",
            "  Batch 26/174 - Loss: 2.4048\n",
            "  Batch 27/174 - Loss: 2.2355\n",
            "  Batch 28/174 - Loss: 2.5211\n",
            "  Batch 29/174 - Loss: 2.4432\n",
            "  Batch 30/174 - Loss: 2.2774\n",
            "  Batch 31/174 - Loss: 2.2442\n",
            "  Batch 32/174 - Loss: 2.2422\n",
            "  Batch 33/174 - Loss: 2.1688\n",
            "  Batch 34/174 - Loss: 2.1693\n",
            "  Batch 35/174 - Loss: 2.4344\n",
            "  Batch 36/174 - Loss: 2.1899\n",
            "  Batch 37/174 - Loss: 2.4312\n",
            "  Batch 38/174 - Loss: 2.1352\n",
            "  Batch 39/174 - Loss: 2.2614\n",
            "  Batch 40/174 - Loss: 2.3639\n",
            "  Batch 41/174 - Loss: 2.4272\n",
            "  Batch 42/174 - Loss: 2.3149\n",
            "  Batch 43/174 - Loss: 2.2818\n",
            "  Batch 44/174 - Loss: 2.2112\n",
            "  Batch 45/174 - Loss: 2.3581\n",
            "  Batch 46/174 - Loss: 2.2985\n",
            "  Batch 47/174 - Loss: 2.2907\n",
            "  Batch 48/174 - Loss: 2.3622\n",
            "  Batch 49/174 - Loss: 2.2633\n",
            "  Batch 50/174 - Loss: 2.1989\n",
            "  Batch 51/174 - Loss: 2.1329\n",
            "  Batch 52/174 - Loss: 2.2956\n",
            "  Batch 53/174 - Loss: 2.2708\n",
            "  Batch 54/174 - Loss: 2.3830\n",
            "  Batch 55/174 - Loss: 2.3500\n",
            "  Batch 56/174 - Loss: 2.2624\n",
            "  Batch 57/174 - Loss: 2.1591\n",
            "  Batch 58/174 - Loss: 2.2713\n",
            "  Batch 59/174 - Loss: 2.3573\n",
            "  Batch 60/174 - Loss: 2.2445\n",
            "  Batch 61/174 - Loss: 2.3757\n",
            "  Batch 62/174 - Loss: 2.2997\n",
            "  Batch 63/174 - Loss: 2.2227\n",
            "  Batch 64/174 - Loss: 2.3625\n",
            "  Batch 65/174 - Loss: 2.2370\n",
            "  Batch 66/174 - Loss: 2.2314\n",
            "  Batch 67/174 - Loss: 2.1562\n",
            "  Batch 68/174 - Loss: 2.2087\n",
            "  Batch 69/174 - Loss: 2.2608\n",
            "  Batch 70/174 - Loss: 2.2428\n",
            "  Batch 71/174 - Loss: 2.1531\n",
            "  Batch 72/174 - Loss: 2.2901\n",
            "  Batch 73/174 - Loss: 2.2822\n",
            "  Batch 74/174 - Loss: 2.2976\n",
            "  Batch 75/174 - Loss: 2.2799\n",
            "  Batch 76/174 - Loss: 2.2657\n",
            "  Batch 77/174 - Loss: 2.2752\n",
            "  Batch 78/174 - Loss: 2.2333\n",
            "  Batch 79/174 - Loss: 2.2716\n",
            "  Batch 80/174 - Loss: 2.1999\n",
            "  Batch 81/174 - Loss: 2.3081\n",
            "  Batch 82/174 - Loss: 2.2381\n",
            "  Batch 83/174 - Loss: 2.0715\n",
            "  Batch 84/174 - Loss: 2.1729\n",
            "  Batch 85/174 - Loss: 2.1672\n",
            "  Batch 86/174 - Loss: 2.1877\n",
            "  Batch 87/174 - Loss: 2.1319\n",
            "  Batch 88/174 - Loss: 2.2201\n",
            "  Batch 89/174 - Loss: 2.0961\n",
            "  Batch 90/174 - Loss: 2.1555\n",
            "  Batch 91/174 - Loss: 2.1382\n",
            "  Batch 92/174 - Loss: 2.1745\n",
            "  Batch 93/174 - Loss: 2.1923\n",
            "  Batch 94/174 - Loss: 2.2075\n",
            "  Batch 95/174 - Loss: 2.1743\n",
            "  Batch 96/174 - Loss: 2.2327\n",
            "  Batch 97/174 - Loss: 2.1611\n",
            "  Batch 98/174 - Loss: 2.3076\n",
            "  Batch 99/174 - Loss: 2.3512\n",
            "  Batch 100/174 - Loss: 2.2595\n",
            "  Batch 101/174 - Loss: 2.1844\n",
            "  Batch 102/174 - Loss: 2.1481\n",
            "  Batch 103/174 - Loss: 2.2778\n",
            "  Batch 104/174 - Loss: 2.1518\n",
            "  Batch 105/174 - Loss: 2.2893\n",
            "  Batch 106/174 - Loss: 2.2484\n",
            "  Batch 107/174 - Loss: 2.1134\n",
            "  Batch 108/174 - Loss: 2.2318\n",
            "  Batch 109/174 - Loss: 2.1132\n",
            "  Batch 110/174 - Loss: 2.0991\n",
            "  Batch 111/174 - Loss: 2.1265\n",
            "  Batch 112/174 - Loss: 2.0757\n",
            "  Batch 113/174 - Loss: 2.2521\n",
            "  Batch 114/174 - Loss: 2.2405\n",
            "  Batch 115/174 - Loss: 2.1229\n",
            "  Batch 116/174 - Loss: 2.2524\n",
            "  Batch 117/174 - Loss: 2.1687\n",
            "  Batch 118/174 - Loss: 2.1104\n",
            "  Batch 119/174 - Loss: 2.2154\n",
            "  Batch 120/174 - Loss: 2.1775\n",
            "  Batch 121/174 - Loss: 2.3089\n",
            "  Batch 122/174 - Loss: 2.1069\n",
            "  Batch 123/174 - Loss: 2.2780\n",
            "  Batch 124/174 - Loss: 2.1271\n",
            "  Batch 125/174 - Loss: 2.1936\n",
            "  Batch 126/174 - Loss: 2.0973\n",
            "  Batch 127/174 - Loss: 2.2714\n",
            "  Batch 128/174 - Loss: 2.0488\n",
            "  Batch 129/174 - Loss: 2.1945\n",
            "  Batch 130/174 - Loss: 2.0764\n",
            "  Batch 131/174 - Loss: 2.1510\n",
            "  Batch 132/174 - Loss: 2.3367\n",
            "  Batch 133/174 - Loss: 2.0311\n",
            "  Batch 134/174 - Loss: 2.2673\n",
            "  Batch 135/174 - Loss: 2.2076\n",
            "  Batch 136/174 - Loss: 2.2141\n",
            "  Batch 137/174 - Loss: 2.1951\n",
            "  Batch 138/174 - Loss: 2.1671\n",
            "  Batch 139/174 - Loss: 2.0810\n",
            "  Batch 140/174 - Loss: 2.0222\n",
            "  Batch 141/174 - Loss: 2.2684\n",
            "  Batch 142/174 - Loss: 2.2305\n",
            "  Batch 143/174 - Loss: 2.1080\n",
            "  Batch 144/174 - Loss: 2.0693\n",
            "  Batch 145/174 - Loss: 2.1491\n",
            "  Batch 146/174 - Loss: 2.2218\n",
            "  Batch 147/174 - Loss: 2.1118\n",
            "  Batch 148/174 - Loss: 2.1086\n",
            "  Batch 149/174 - Loss: 2.1980\n",
            "  Batch 150/174 - Loss: 2.0997\n",
            "  Batch 151/174 - Loss: 1.9912\n",
            "  Batch 152/174 - Loss: 2.0980\n",
            "  Batch 153/174 - Loss: 2.0633\n",
            "  Batch 154/174 - Loss: 1.9739\n",
            "  Batch 155/174 - Loss: 2.0928\n",
            "  Batch 156/174 - Loss: 2.1702\n",
            "  Batch 157/174 - Loss: 2.0600\n",
            "  Batch 158/174 - Loss: 2.1268\n",
            "  Batch 159/174 - Loss: 2.1117\n",
            "  Batch 160/174 - Loss: 2.1929\n",
            "  Batch 161/174 - Loss: 2.0831\n",
            "  Batch 162/174 - Loss: 2.1813\n",
            "  Batch 163/174 - Loss: 2.0792\n",
            "  Batch 164/174 - Loss: 2.0097\n",
            "  Batch 165/174 - Loss: 2.0928\n",
            "  Batch 166/174 - Loss: 2.2713\n",
            "  Batch 167/174 - Loss: 2.2422\n",
            "  Batch 168/174 - Loss: 2.1483\n",
            "  Batch 169/174 - Loss: 2.1354\n",
            "  Batch 170/174 - Loss: 2.1016\n",
            "  Batch 171/174 - Loss: 2.0230\n",
            "  Batch 172/174 - Loss: 2.0585\n",
            "  Batch 173/174 - Loss: 2.2287\n",
            "  Batch 174/174 - Loss: 2.2938\n",
            "Epoch 2 - Average Training Loss: 2.2244\n",
            "Epoch 2 - Average Validation Loss: 2.0993\n",
            "Epoch 3/3 - Training Phase\n",
            "  Batch 1/174 - Loss: 2.1328\n",
            "  Batch 2/174 - Loss: 2.2006\n",
            "  Batch 3/174 - Loss: 2.2142\n",
            "  Batch 4/174 - Loss: 2.1598\n",
            "  Batch 5/174 - Loss: 2.0585\n",
            "  Batch 6/174 - Loss: 2.1587\n",
            "  Batch 7/174 - Loss: 2.1017\n",
            "  Batch 8/174 - Loss: 2.1299\n",
            "  Batch 9/174 - Loss: 2.1614\n",
            "  Batch 10/174 - Loss: 2.0476\n",
            "  Batch 11/174 - Loss: 2.1581\n",
            "  Batch 12/174 - Loss: 2.0575\n",
            "  Batch 13/174 - Loss: 2.0911\n",
            "  Batch 14/174 - Loss: 2.1233\n",
            "  Batch 15/174 - Loss: 2.0338\n",
            "  Batch 16/174 - Loss: 2.3101\n",
            "  Batch 17/174 - Loss: 2.2142\n",
            "  Batch 18/174 - Loss: 1.9785\n",
            "  Batch 19/174 - Loss: 2.0853\n",
            "  Batch 20/174 - Loss: 2.1682\n",
            "  Batch 21/174 - Loss: 1.9857\n",
            "  Batch 22/174 - Loss: 2.0389\n",
            "  Batch 23/174 - Loss: 2.1058\n",
            "  Batch 24/174 - Loss: 2.1177\n",
            "  Batch 25/174 - Loss: 2.1941\n",
            "  Batch 26/174 - Loss: 1.8825\n",
            "  Batch 27/174 - Loss: 2.0426\n",
            "  Batch 28/174 - Loss: 1.9752\n",
            "  Batch 29/174 - Loss: 2.0328\n",
            "  Batch 30/174 - Loss: 2.0581\n",
            "  Batch 31/174 - Loss: 2.0415\n",
            "  Batch 32/174 - Loss: 2.1271\n",
            "  Batch 33/174 - Loss: 2.0935\n",
            "  Batch 34/174 - Loss: 2.0806\n",
            "  Batch 35/174 - Loss: 1.8935\n",
            "  Batch 36/174 - Loss: 2.1474\n",
            "  Batch 37/174 - Loss: 2.0449\n",
            "  Batch 38/174 - Loss: 2.2121\n",
            "  Batch 39/174 - Loss: 2.0873\n",
            "  Batch 40/174 - Loss: 1.8974\n",
            "  Batch 41/174 - Loss: 2.1451\n",
            "  Batch 42/174 - Loss: 1.9521\n",
            "  Batch 43/174 - Loss: 2.2861\n",
            "  Batch 44/174 - Loss: 2.1212\n",
            "  Batch 45/174 - Loss: 1.9787\n",
            "  Batch 46/174 - Loss: 2.1518\n",
            "  Batch 47/174 - Loss: 1.9691\n",
            "  Batch 48/174 - Loss: 2.0304\n",
            "  Batch 49/174 - Loss: 2.2850\n",
            "  Batch 50/174 - Loss: 2.0151\n",
            "  Batch 51/174 - Loss: 2.0665\n",
            "  Batch 52/174 - Loss: 1.9925\n",
            "  Batch 53/174 - Loss: 1.9044\n",
            "  Batch 54/174 - Loss: 2.1854\n",
            "  Batch 55/174 - Loss: 1.9806\n",
            "  Batch 56/174 - Loss: 2.2061\n",
            "  Batch 57/174 - Loss: 1.9862\n",
            "  Batch 58/174 - Loss: 1.9913\n",
            "  Batch 59/174 - Loss: 2.0618\n",
            "  Batch 60/174 - Loss: 2.0692\n",
            "  Batch 61/174 - Loss: 2.1144\n",
            "  Batch 62/174 - Loss: 2.0590\n",
            "  Batch 63/174 - Loss: 1.9110\n",
            "  Batch 64/174 - Loss: 2.1593\n",
            "  Batch 65/174 - Loss: 2.0789\n",
            "  Batch 66/174 - Loss: 2.0664\n",
            "  Batch 67/174 - Loss: 2.1098\n",
            "  Batch 68/174 - Loss: 2.0381\n",
            "  Batch 69/174 - Loss: 2.0346\n",
            "  Batch 70/174 - Loss: 2.0185\n",
            "  Batch 71/174 - Loss: 2.0045\n",
            "  Batch 72/174 - Loss: 1.8634\n",
            "  Batch 73/174 - Loss: 2.0573\n",
            "  Batch 74/174 - Loss: 2.0228\n",
            "  Batch 75/174 - Loss: 2.1911\n",
            "  Batch 76/174 - Loss: 1.9325\n",
            "  Batch 77/174 - Loss: 2.0289\n",
            "  Batch 78/174 - Loss: 2.1641\n",
            "  Batch 79/174 - Loss: 2.0848\n",
            "  Batch 80/174 - Loss: 1.8739\n",
            "  Batch 81/174 - Loss: 2.0244\n",
            "  Batch 82/174 - Loss: 2.0645\n",
            "  Batch 83/174 - Loss: 2.0111\n",
            "  Batch 84/174 - Loss: 2.1907\n",
            "  Batch 85/174 - Loss: 1.9963\n",
            "  Batch 86/174 - Loss: 2.0694\n",
            "  Batch 87/174 - Loss: 2.0664\n",
            "  Batch 88/174 - Loss: 2.1556\n",
            "  Batch 89/174 - Loss: 1.9553\n",
            "  Batch 90/174 - Loss: 2.0629\n",
            "  Batch 91/174 - Loss: 1.9431\n",
            "  Batch 92/174 - Loss: 2.0030\n",
            "  Batch 93/174 - Loss: 2.0058\n",
            "  Batch 94/174 - Loss: 2.1133\n",
            "  Batch 95/174 - Loss: 2.0722\n",
            "  Batch 96/174 - Loss: 2.1454\n",
            "  Batch 97/174 - Loss: 2.0565\n",
            "  Batch 98/174 - Loss: 1.9524\n",
            "  Batch 99/174 - Loss: 2.0271\n",
            "  Batch 100/174 - Loss: 2.0903\n",
            "  Batch 101/174 - Loss: 2.0500\n",
            "  Batch 102/174 - Loss: 2.0732\n",
            "  Batch 103/174 - Loss: 2.0513\n",
            "  Batch 104/174 - Loss: 1.9480\n",
            "  Batch 105/174 - Loss: 2.0925\n",
            "  Batch 106/174 - Loss: 1.9974\n",
            "  Batch 107/174 - Loss: 1.9565\n",
            "  Batch 108/174 - Loss: 2.1172\n",
            "  Batch 109/174 - Loss: 2.2463\n",
            "  Batch 110/174 - Loss: 2.0278\n",
            "  Batch 111/174 - Loss: 1.9551\n",
            "  Batch 112/174 - Loss: 2.0053\n",
            "  Batch 113/174 - Loss: 2.1685\n",
            "  Batch 114/174 - Loss: 2.0600\n",
            "  Batch 115/174 - Loss: 1.9016\n",
            "  Batch 116/174 - Loss: 1.9334\n",
            "  Batch 117/174 - Loss: 1.9889\n",
            "  Batch 118/174 - Loss: 2.1525\n",
            "  Batch 119/174 - Loss: 2.0545\n",
            "  Batch 120/174 - Loss: 2.0872\n",
            "  Batch 121/174 - Loss: 1.8981\n",
            "  Batch 122/174 - Loss: 2.0125\n",
            "  Batch 123/174 - Loss: 2.1124\n",
            "  Batch 124/174 - Loss: 1.9323\n",
            "  Batch 125/174 - Loss: 2.1401\n",
            "  Batch 126/174 - Loss: 2.1331\n",
            "  Batch 127/174 - Loss: 2.0681\n",
            "  Batch 128/174 - Loss: 1.8556\n",
            "  Batch 129/174 - Loss: 1.9739\n",
            "  Batch 130/174 - Loss: 1.9423\n",
            "  Batch 131/174 - Loss: 1.9350\n",
            "  Batch 132/174 - Loss: 2.0960\n",
            "  Batch 133/174 - Loss: 2.1341\n",
            "  Batch 134/174 - Loss: 2.0224\n",
            "  Batch 135/174 - Loss: 1.8956\n",
            "  Batch 136/174 - Loss: 1.8989\n",
            "  Batch 137/174 - Loss: 1.9340\n",
            "  Batch 138/174 - Loss: 2.0085\n",
            "  Batch 139/174 - Loss: 1.8534\n",
            "  Batch 140/174 - Loss: 1.9210\n",
            "  Batch 141/174 - Loss: 2.0586\n",
            "  Batch 142/174 - Loss: 2.1013\n",
            "  Batch 143/174 - Loss: 2.0234\n",
            "  Batch 144/174 - Loss: 1.9105\n",
            "  Batch 145/174 - Loss: 1.9986\n",
            "  Batch 146/174 - Loss: 2.1005\n",
            "  Batch 147/174 - Loss: 2.0298\n",
            "  Batch 148/174 - Loss: 1.9750\n",
            "  Batch 149/174 - Loss: 2.0749\n",
            "  Batch 150/174 - Loss: 2.0450\n",
            "  Batch 151/174 - Loss: 1.8787\n",
            "  Batch 152/174 - Loss: 2.1001\n",
            "  Batch 153/174 - Loss: 2.0501\n",
            "  Batch 154/174 - Loss: 2.0753\n",
            "  Batch 155/174 - Loss: 1.9802\n",
            "  Batch 156/174 - Loss: 1.9644\n",
            "  Batch 157/174 - Loss: 1.8832\n",
            "  Batch 158/174 - Loss: 2.0374\n",
            "  Batch 159/174 - Loss: 1.8636\n",
            "  Batch 160/174 - Loss: 2.0284\n",
            "  Batch 161/174 - Loss: 1.9952\n",
            "  Batch 162/174 - Loss: 2.1207\n",
            "  Batch 163/174 - Loss: 1.9987\n",
            "  Batch 164/174 - Loss: 1.9987\n",
            "  Batch 165/174 - Loss: 2.2160\n",
            "  Batch 166/174 - Loss: 2.0752\n",
            "  Batch 167/174 - Loss: 2.0167\n",
            "  Batch 168/174 - Loss: 1.9453\n",
            "  Batch 169/174 - Loss: 1.9596\n",
            "  Batch 170/174 - Loss: 1.9053\n",
            "  Batch 171/174 - Loss: 1.9826\n",
            "  Batch 172/174 - Loss: 2.0149\n",
            "  Batch 173/174 - Loss: 1.9814\n",
            "  Batch 174/174 - Loss: 2.1764\n",
            "Epoch 3 - Average Training Loss: 2.0462\n",
            "Epoch 3 - Average Validation Loss: 1.9824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5pV_7izut7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}