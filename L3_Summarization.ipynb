{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucaokylong/LLM_learning/blob/main/L3_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftuyx87CNBf4"
      },
      "outputs": [],
      "source": [
        "openai_api_key = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cài các thư viện liên quan"
      ],
      "metadata": {
        "id": "KIgLgKspNdt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet -U langchain chromadb langchain-openai pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91KTtzNCPY9T",
        "outputId": "7b09a1ff-37b1-458d-f8aa-0d16e6be4d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[]"
      ],
      "metadata": {
        "id": "nV7Rh1QgWKyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tóm tắt"
      ],
      "metadata": {
        "id": "FrJi7CleQpKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Remove leading/trailing whitespace and split the content into sentences\n",
        "    sentences = content.strip().split('.')\n",
        "\n",
        "    # Remove empty sentences\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    # Calculate the number of sentences per chunk\n",
        "    total_sentences = len(sentences)\n",
        "    chunk_size = total_sentences // 10\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    for i in range(9):\n",
        "        end = start + chunk_size\n",
        "        chunk = sentences[start:end]\n",
        "        chunks.append('. '.join(chunk) + '.')\n",
        "        start = end\n",
        "\n",
        "    # Add the remaining sentences to the last chunk\n",
        "    last_chunk = sentences[start:]\n",
        "    chunks.append('. '.join(last_chunk) + '.')\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/vinallama.txt'\n",
        "chunks = split_file(file_path)\n",
        "\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    print(chunk)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_mMptRmPgti",
        "outputId": "e4932266-b409-4610-eae6-8ff6099e49ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "In this technical report, we present VinaLLaMA, an open-source, state-of-the-art (SOTA) Large Language Model for the Vietnamese language, built upon LLaMA-2 with an additional 800 billion trained tokens. VinaLLaMA not only demonstrates fluency in Vietnamese but also exhibits a profound understanding of Vietnamese culture, making it a truly indigenous model. VinaLLaMA-7B-chat, trained on 1-million high quality synthetic samples, achieves SOTA results on key benchmarks, including VLSP, VMLU, and Vicuna Benchmark Vietnamese, marking a significant advancement in the Vietnamese AI landscape and offering a versatile resource for various applications. 1 Introduction \n",
            "The surge in Large Language Models (LLMs) such as ChatGPT and GPT-4 has significantly advanced the field of artificial intelligence (AI), particularly in language processing. In 2023, Vietnam’s AI sector witnessed a notable development with the introduction of several Vietnamese-centric LLMs, including BLOOMZ’s Vietcuna, URA-LLaMA, PhoGPT, and dama-2. Amidst this progression, we introduce VinaLLaMA, a foundational LLM designed specifically for the Vietnamese language. VinaL- LaMA, built on top of LLaMA-2, represents a vital stride towards linguistic inclusivity in AI, adeptly addressing the syntactic and semantic intricacies of Vietnamese. Embracing the spirit of collaboration and open innovation, we are pleased to announce the open sourcing of VinaLLaMA-7B and its chat variant. These models are now accessible on HuggingFace, ensuring compatibility with all ’transformers’ framework-supported libraries. This endeavor not only contributes to the global AI research landscape but also provides a specialized tool for exploring and enhancing Vietnamese language processing, encouraging a wider engagement and application in AI- driven NLP research. 2 Related Work2. 1 Large Language Models. The growth of large language models (LLMs) start from Transformer [VSP+17], which was the base architecture to later pre-train BERT [CMCC23] and GPT [RNSS] with large-scale unsupervised data. These models lead to the appearance of popular foundation models RoBERTa [LOG+19], T5 [RSR+20] and BART [LLG+20]. Later, GPT-3 [BMR+20] perform the ability of few-shot and zero-shot learning through prompt engineering and in-context learning.\n",
            "\n",
            "Chunk 2:\n",
            "Later, ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) are the two big milestone for not only in the field of language model, but also in aritifical intelligence and other fields. These models show the variety of skills with high quality output. These also lead to the raise of LLMs, which includes Llama [RSR+20], Llama-2 [TMS+23], Bloom [WSF+22], Falcon [AAA+23], Qwen [BBC+23] and Mistral [JSM+23]. 2. 2 Vietnamese Large Language Models. The landscape of Vietnamese Large Language Models has been evolving with significant contributions. Vietcuna [NPD23b] emerged as the pioneering model in this domain, initially undergoing further pre-training on the BLOOMZ [WSF+22] model. It was subsequently enhanced through fine-tuning with the OpenOrca-Viet [NPD23a] dataset, optimizing its performance in Vietnamese language tasks. In a different approach, URA-Llama [NTN+23] was developed, leveraging fine-tuning on a corpus of Vietnamese articles, building upon the foundational architecture of Llama-2. Furthermore, the PhoGPT [NNT+23] model represents another notable advancement. It was initially pre-trained on a substantial corpus of 41GB of text data, followed by a focused fine-tuning process utilizing a dataset of 150,000 samples, thereby broadening its capabilities in understanding and generating Vietnamese text. 2. 3 Alignment. To enhance the ability of language model, including in-context learning, reasoning, instruction learn- ing,. we need to align the model with some downstream dataset and supervised fine-tuning the LLMs on these dataset.\n",
            "\n",
            "Chunk 3:\n",
            "Flan [WBZ+21] introduce instruction tuning and release the first public dataset which includes huge amount of different tasks with different templates. This improve the general performance of LLms a lot. Later, InstructGPT [OWJ+22] apply human feedback to modify the model output, help the model to generate the text with less toxic and higher quality, compare to other models. Vicuna and Guanaco include chat data from open-assistant dataset, which enhacne the chat ability of the model. Recently, since the ability of GPT-3,5 and GPT-4, many synthetic dataset that are generated from these models have been popularly applied such as: OpenOrca [WKM+22b], Dolphin [WKM+22a], OpenOrca-2 [WKM+22c], Platypus [LHR23]. 3 Training Procedure 3. 1 Pretraining \n",
            "LLaMA-2, a highly regarded pre-trained language model in English, shows a significant gap in handling Vietnamese-related content due to limited relevant tokens in its training set. Additionally, its original tokenizer falls short in multilingual applications. To address these issues, we compiled a new pretraining dataset combining public and synthetic in-house data. We selected BKAI’s LLaMA-2-chat tokenizer for the tokenizer, which has been specifically made for Vietnamese. This tokenizer shows enhanced performance in processing the Vietnamese language, making it a suitable choice for our VinaLLaMA model. 3. 1. 1 Public Data \n",
            "Books. The dataset encompassing Vietnamese literature in our study is comprehensive, comprising 250,000 volumes.\n",
            "\n",
            "Chunk 4:\n",
            "This extensive collection spans various domains, including science, history, finance, and philosophy, as well as fiction genres like novels and science fiction, in addition to traditional Viet- namese literature. The distribution of these categories is methodically illustrated in Figure 1. Public News. The dataset under examination is derived from two principal Vietnamese news sources, VnExpress 1 and BaoMoi 2. This compilation encompasses an exhaustive collection of articles dis- seminated by these entities from January 1, 2010, through September 30, 2023. To align with ethical guidelines and content appropriateness standards, a filtration process was implemented, systematically excluding articles that contained keywords indicative of harmful or violent content. The inclusion of this public news data is crucial, as it helps the model better understand and represent important aspects related to Vietnam and its people. Finally, we also include a subset of Vietnamese from CulturaX and an additional 100B tokens in English. The final public dataset has a total of roughly 330 billion tokens. 3. 1. 2 In-house Data \n",
            "Influenced by the concept of synthetic textbooks for pretraining [GZA+23], our approach incorporates a mechanism that selects random text segments from a publicly available dataset. These segments are then integrated into over 80 bespoke prompt templates, each meticulously crafted to facilitate the rewriting task. The prompts, when fed into GPT-4, result in roughly 100,000 samples. These samples represent a high-quality synthetic dataset, specifically tailored for pretraining purposes.\n",
            "\n",
            "Chunk 5:\n",
            "The methodology and workflow of this process are illustrated in Figure 2. Additionally, we employed Vietcuna-3B-v2, our successor smaller-scale language model, to train on the synthetic samples generated in Step 1. This training process utilized the rewriting task-specific prompts previously developed (as outlined in Step 2. 1), which is detailed in Figure 3. Subsequently, we replicated the procedure from Step 1 using this newly trained model. This iteration resulted in the generation of over 500 billion synthetic tokens, a process detailed in Step 2. 2. The final result of this process is morning 500 bilions of high quality Vietnamese tokens ready to be used to continue pretrain on the base LLaMA 2 with the expanded tokenizer. 3. 2 Supervised Instruction Fine-tuning \n",
            "In collaboration with Nous Research 3, we employed proprietary methodologies to create 500,000 Viet- namese synthetic samples. These samples encompassed both instructional and conversational formats. To enhance the dataset, an additional 500,000 English samples were sourced from the OpenHermes-2. 5 [tek23] and Capybara [DS23] datasets, also provided by Nous Research. This integration of datasets culminated in a comprehensive collection of 1 million samples, paving the way for the development of a bilingual language model. The comprehensive final dataset encompasses an array of tasks, including reasoning, role-playing, poem writing, coding, function calling, and agent prompting.\n",
            "\n",
            "Chunk 6:\n",
            "This diversity ensures a broad scope of capabilities in the final model. We conducted a full fine-tuning of our pre- trained model over four epochs, which led to the creation of a state-of-the-art foundation language model specifically tailored for the Vietnamese language, more on the result in Section 4. 3. 3 VinaLLaMA-2. 7B \n",
            "Adopting the structured pruning procedure outlined in [XGZC23], we successfully reduced our model to a more compact variant with 2. 7 billion parameters. This process involved strategically pruning the network while retaining its core functional capabilities. Following the reduction in size, we applied the same supervised fine-tuning methodology to this smaller variant as was used for the 7B model, ensuring consistency in training approach across different model scales. 4 Evaluation \n",
            "In our research, we utilized three distinct evaluation benchmarks: VLSP, VMLU, and the Vicuna Benchmark, with the latter being translated into Vietnamese by VinAI Research. For each of these benchmarks, we implemented a dual-evaluation approach. Specifically, we conducted separate exper- iments for two categories of models: those in their pre-trained state and those that had undergone instructional fine-tuning. This methodology allowed us to compare the baseline capabilities of the pre-trained models against their performance post-instructional fine-tuning, providing insights into the effectiveness of fine-tuning in enhancing model proficiency within the context of these Vietnamese language benchmarks. For the pre-training benchmarks, we selected VLSP’s hoa-7b, BLOOM-7B, and PhoGPT-7B5 as the base models. In contrast, the instructional fine-tuned benchmarks comprised BLOOMZ-7B, Vietcuna- 7B-v3, PhoGPT-7B5-Instruct, URA-LLaMA-13B and SeaLLM-7B-chat. This diverse set of models provided a comprehensive perspective, enabling us to assess both the inherent capabilities of these models in their original form and their enhanced performance post-fine-tuning, specifically tailored to the Vietnamese language benchmarks.\n",
            "\n",
            "Chunk 7:\n",
            "4. 1 VLSP \n",
            "The first benchmark employed is the VLSP-LLM 2023 [CHC+23]. This benchmark parallels the Open- LLM Leaderboard by HuggingFace, tailored specifically for the Vietnamese language. It encompasses four distinct assessments: ARC Challenge, HellaSwag, MMLU, and TruthfulQA, each meticulously adapted to Vietnamese. This comprehensive benchmark suite allows for a robust evaluation of lan- guage models in understanding and generating Vietnamese text across various domains and complex- ities. The results are reported in Table 1 and Table 2. In the pretrained segment of our study, the VinaLLaMA-7B model demonstrated superior performance compared to other open-source Large Lan- guage Models (LLMs) that support Vietnamese. This was evident in its outperformance on three of the four benchmarks, leading to it achieving state-of-the-art results based on the average score across these benchmarks. This signifies the robustness and effectiveness of the VinaLLaMA-7B model in handling a variety of tasks in the Vietnamese language context. In the supervised fine-tuning domain, our results continued to reflect a high level of excellence. No- tably, VinaLLaMA-7B-chat, comparable in scale, astonishingly outperformed larger models, including those with 13 billion parameters, in terms of average score. This impressive achievement highlights the efficacy of the fine-tuning process and the model’s adeptness at leveraging its training to deliver outstanding performance, even against much larger counterparts. Adding to this, the VinaLLaMA- 2. 7B, a significantly smaller model, showcased remarkable performance. It not only competed closely with larger 7B variants but also exceeded PhoGPT-7B5-Instruct in performance, while achieving in- ferencing speeds 60% faster.\n",
            "\n",
            "Chunk 8:\n",
            "This showcases the model’s optimized balance between size, speed, and performance. The success of VinaLLaMA-7B-chat, and the noteworthy performance of VinaLLaMA- 2. 7B, underscore the considerable potential of well-implemented fine-tuning strategies with synthetic data in elevating the capabilities of Large Language Models, especially in contexts requiring specialized language processing. 4. 2 VMLU \n",
            "VMLU [AoST23], a benchmark suite tailored for evaluating foundation models in the Vietnamese language, comprises 10,880 multiple-choice questions across 58 subjects in domains like STEM, Hu- manities, and Social Sciences. Its diverse range of difficulty levels tests models from basic knowledge to advanced problem-solving. We selected VMLU for benchmarking due to its comprehensive coverage of Vietnam-related questions, providing a relevant and challenging environment for assessing model ca- pabilities in a context-specific manner. We conducted our experiments on the validation set of VMLU since the answers to test set are not publicly available, URA-LLaMA-13B is also not being test due to the lack of availability at testing time. We reported the results under two few-shot settings: 0-shot and 5-shot, which can be viewed in Table 3 and 4. 4. 3 Vicuna Benchmark Vietnamese \n",
            "The Vicuna Benchmark [ZCS+23], translated into Vietnamese by VinAI, serves as our final bench- mark. This comprehensive benchmark is composed of 80 distinct instructions spanning 9 diverse areas, providing a broad spectrum for assessing model capabilities. Uniquely, the evaluation of the results from all participating models is conducted using GPT-4, which introduces an innovative approach to performance assessment. This methodology employs an ELO ranking system, traditionally used in chess and other competitive games, to rate the models. Such a system offers a dynamic and rela- tive measure of model performance, allowing for a nuanced and comparative analysis of each model’s proficiency in handling a variety of tasks and instructions within the benchmark.\n",
            "\n",
            "Chunk 9:\n",
            "This ELO-based evaluation provides a clear and quantifiable ranking of the models, reflecting their effectiveness and adaptability in the context of the Vietnamese language and the specific challenges presented by the Vicuna Benchmark. In our Vicuna Benchmark evaluation, responses from models were assessed using a detailed five- point scale: 0 (’very bad’), 1 (’bad’), 2 (’ok’), 3 (’good’), and 4 (’very good’). This granular scoring system allows for an in-depth evaluation of the quality of each model’s response. The final ELO score for each model is computed by aggregating these individual ratings, providing a holistic measure of a model’s overall performance across the benchmark’s varied tasks. To ensure language relevance, we implemented a strict rule: any response not in Vietnamese is automatically assigned a score of 0. This criterion underscores the importance of language-specific accuracy in our evaluation. The collective results, reflecting model performances across different tasks, are visually represented in Figure 4. In the interest of transparency and further research, we have made VinaLLaMA’s responses and our evaluation code publicly available 4 5. This not only allows for independent verification of our results but also facilitates further advancements in the field by providing valuable resources to other researchers. The benchmark results revealed that VinaLLaMA showcased commendable performance in Viet- namese, closely trailing behind ChatGPT-3. 5-Turbo in some benchmarks. This indicates that VinaLLaMA is highly effective in Vietnamese language tasks, with only a slight margin separating it from the more advanced ChatGPT-3. 5-Turbo, as shown in Table 5 and Table 6. PhoGPT also demonstrated better outcomes compared to other Vietnamese benchmarks, yet it still fell significantly behind in some areas, highlighting potential avenues for improvement. In contrast, URA-LLaMA-7B and 13B models performed less effectively, with some of their re- sponses not aligning with Vietnamese language requirements.\n",
            "\n",
            "Chunk 10:\n",
            "This points to a critical need for targeted enhancements or language-specific training for these models to better cater to Vietnamese language processing. 5 Acknowledgement \n",
            "We are also immensely grateful to the teams at Nous Research, LAION, FPT Software AI Center, and Symato Team. Their assistance in the early stages of VinaLLaMA’s evaluation was crucial, providing insights that guided further improvements and enhancements of the model. We also want to send our appreciation to Nam Pham and Nhan Nguyen for the contribution in our public dataset. Lastly, our profound appreciation is extended to Google Cloud and Stability. AI. Their generous computational support was a cornerstone in bringing VinaLLaMA to fruition, enabling the intensive training and development processes required for such a sophisticated large language model. Their contribution was vital in transforming our vision for VinaLLaMA into a reality. 6 Conclusion \n",
            "In conclusion, the development of VinaLLaMA marks a significant milestone in the realm of language models, particularly in the context of Vietnamese language processing. Achieving state-of-the-art (SOTA) scores across all Vietnamese benchmarks, VinaLLaMA has demonstrated exceptional profi- ciency and adaptability. While its performance in English benchmarks was slightly less dominant, it still showed considerable competence, underscoring its effectiveness as a bilingual model. A key factor in VinaLLaMA’s success is the strategic use of carefully crafted synthetic data in its training regimen. This approach has proven to be highly effective, yielding impressive results and highlighting the importance of diverse and well-designed training datasets in the development of robust language models. VinaLLaMA’s achievements not only set a new standard for language models in Vietnamese but also contribute valuable insights into the broader field of natural language processing. It exemplifies how meticulous data preparation and comprehensive training can significantly enhance the capabilities of language models, paving the way for future advancements in this dynamic and rapidly evolving domain.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization (Tóm Tắt) là một trong số những task khó nhất cho các LLM. Nó đỏi hỏi LLM có khả năng xử lí long context nhưng vẫn có khả năng làm retrieval trên text tốt. Vì vậy, đối vơi đại đa số các LLM, chúng ta sẽ làm prompt chaining để triển khai bài toán này"
      ],
      "metadata": {
        "id": "Sxw3NfhWQs5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Hãy tóm tắt văn bản: {context}\n",
        "\"\"\"\n",
        "\n",
        "summarize_prompt = \"\"\"\n",
        "Sau đây là một nội dung được trích xuất của một văn bản, nhiệm vụ của bạn là hãy tóm tắt nó\n",
        "---CONTEXT---\n",
        "{context}\n",
        "---END CONTEXT---\n",
        "Hãy đưa ra tóm tắt của văn bản trên. Tôi chỉ cần phần tóm tắt, và không cần thêm bất kì thứ gì khác.\n",
        "Tóm Tắt:\n",
        "\"\"\"\n",
        "\n",
        "final_ans_prompt = \"\"\"\n",
        "Dưới đây là các đoạn tóm tắt của từng đoạn nhỏ của một văn bản lớn.\n",
        "---CONTEXT---\n",
        "{context}\n",
        "---END CONTEXT---\n",
        "Dựa trên các đoạn trên, hãy đưa ra tóm tắt tổng của tất cả các đoạn trên. Tôi chỉ cần tóm tắt tổng, không cần thêm bất kì thứ gì khác.\n",
        "Tóm tắt tổng:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eewqw1OlP-Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_final_context(chunks):\n",
        "    context = \"\"\n",
        "    for index, chunk in enumerate(chunks):\n",
        "        context +=  f\"Context {index + 1}: \" + chunk + \"\\n\"\n",
        "    return context"
      ],
      "metadata": {
        "id": "fWqbSUTHSuuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_final_context(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "vYjBek9HS67S",
        "outputId": "fe4dbd1a-578f-4386-bf66-0ec0b96840af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Context 1: In this technical report, we present VinaLLaMA, an open-source, state-of-the-art (SOTA) Large Language Model for the Vietnamese language, built upon LLaMA-2 with an additional 800 billion trained tokens. VinaLLaMA not only demonstrates fluency in Vietnamese but also exhibits a profound understanding of Vietnamese culture, making it a truly indigenous model. VinaLLaMA-7B-chat, trained on 1-million high quality synthetic samples, achieves SOTA results on key benchmarks, including VLSP, VMLU, and Vicuna Benchmark Vietnamese, marking a significant advancement in the Vietnamese AI landscape and offering a versatile resource for various applications. 1 Introduction \\nThe surge in Large Language Models (LLMs) such as ChatGPT and GPT-4 has significantly advanced the field of artificial intelligence (AI), particularly in language processing. In 2023, Vietnam’s AI sector witnessed a notable development with the introduction of several Vietnamese-centric LLMs, including BLOOMZ’s Vietcuna, URA-LLaMA, PhoGPT, and dama-2. Amidst this progression, we introduce VinaLLaMA, a foundational LLM designed specifically for the Vietnamese language. VinaL- LaMA, built on top of LLaMA-2, represents a vital stride towards linguistic inclusivity in AI, adeptly addressing the syntactic and semantic intricacies of Vietnamese. Embracing the spirit of collaboration and open innovation, we are pleased to announce the open sourcing of VinaLLaMA-7B and its chat variant. These models are now accessible on HuggingFace, ensuring compatibility with all ’transformers’ framework-supported libraries. This endeavor not only contributes to the global AI research landscape but also provides a specialized tool for exploring and enhancing Vietnamese language processing, encouraging a wider engagement and application in AI- driven NLP research. 2 Related Work2. 1 Large Language Models. The growth of large language models (LLMs) start from Transformer [VSP+17], which was the base architecture to later pre-train BERT [CMCC23] and GPT [RNSS] with large-scale unsupervised data. These models lead to the appearance of popular foundation models RoBERTa [LOG+19], T5 [RSR+20] and BART [LLG+20]. Later, GPT-3 [BMR+20] perform the ability of few-shot and zero-shot learning through prompt engineering and in-context learning.\\nContext 2: Later, ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) are the two big milestone for not only in the field of language model, but also in aritifical intelligence and other fields. These models show the variety of skills with high quality output. These also lead to the raise of LLMs, which includes Llama [RSR+20], Llama-2 [TMS+23], Bloom [WSF+22], Falcon [AAA+23], Qwen [BBC+23] and Mistral [JSM+23]. 2. 2 Vietnamese Large Language Models. The landscape of Vietnamese Large Language Models has been evolving with significant contributions. Vietcuna [NPD23b] emerged as the pioneering model in this domain, initially undergoing further pre-training on the BLOOMZ [WSF+22] model. It was subsequently enhanced through fine-tuning with the OpenOrca-Viet [NPD23a] dataset, optimizing its performance in Vietnamese language tasks. In a different approach, URA-Llama [NTN+23] was developed, leveraging fine-tuning on a corpus of Vietnamese articles, building upon the foundational architecture of Llama-2. Furthermore, the PhoGPT [NNT+23] model represents another notable advancement. It was initially pre-trained on a substantial corpus of 41GB of text data, followed by a focused fine-tuning process utilizing a dataset of 150,000 samples, thereby broadening its capabilities in understanding and generating Vietnamese text. 2. 3 Alignment. To enhance the ability of language model, including in-context learning, reasoning, instruction learn- ing,. we need to align the model with some downstream dataset and supervised fine-tuning the LLMs on these dataset.\\nContext 3: Flan [WBZ+21] introduce instruction tuning and release the first public dataset which includes huge amount of different tasks with different templates. This improve the general performance of LLms a lot. Later, InstructGPT [OWJ+22] apply human feedback to modify the model output, help the model to generate the text with less toxic and higher quality, compare to other models. Vicuna and Guanaco include chat data from open-assistant dataset, which enhacne the chat ability of the model. Recently, since the ability of GPT-3,5 and GPT-4, many synthetic dataset that are generated from these models have been popularly applied such as: OpenOrca [WKM+22b], Dolphin [WKM+22a], OpenOrca-2 [WKM+22c], Platypus [LHR23]. 3 Training Procedure 3. 1 Pretraining \\nLLaMA-2, a highly regarded pre-trained language model in English, shows a significant gap in handling Vietnamese-related content due to limited relevant tokens in its training set. Additionally, its original tokenizer falls short in multilingual applications. To address these issues, we compiled a new pretraining dataset combining public and synthetic in-house data. We selected BKAI’s LLaMA-2-chat tokenizer for the tokenizer, which has been specifically made for Vietnamese. This tokenizer shows enhanced performance in processing the Vietnamese language, making it a suitable choice for our VinaLLaMA model. 3. 1. 1 Public Data \\nBooks. The dataset encompassing Vietnamese literature in our study is comprehensive, comprising 250,000 volumes.\\nContext 4: This extensive collection spans various domains, including science, history, finance, and philosophy, as well as fiction genres like novels and science fiction, in addition to traditional Viet- namese literature. The distribution of these categories is methodically illustrated in Figure 1. Public News. The dataset under examination is derived from two principal Vietnamese news sources, VnExpress 1 and BaoMoi 2. This compilation encompasses an exhaustive collection of articles dis- seminated by these entities from January 1, 2010, through September 30, 2023. To align with ethical guidelines and content appropriateness standards, a filtration process was implemented, systematically excluding articles that contained keywords indicative of harmful or violent content. The inclusion of this public news data is crucial, as it helps the model better understand and represent important aspects related to Vietnam and its people. Finally, we also include a subset of Vietnamese from CulturaX and an additional 100B tokens in English. The final public dataset has a total of roughly 330 billion tokens. 3. 1. 2 In-house Data \\nInfluenced by the concept of synthetic textbooks for pretraining [GZA+23], our approach incorporates a mechanism that selects random text segments from a publicly available dataset. These segments are then integrated into over 80 bespoke prompt templates, each meticulously crafted to facilitate the rewriting task. The prompts, when fed into GPT-4, result in roughly 100,000 samples. These samples represent a high-quality synthetic dataset, specifically tailored for pretraining purposes.\\nContext 5: The methodology and workflow of this process are illustrated in Figure 2. Additionally, we employed Vietcuna-3B-v2, our successor smaller-scale language model, to train on the synthetic samples generated in Step 1. This training process utilized the rewriting task-specific prompts previously developed (as outlined in Step 2. 1), which is detailed in Figure 3. Subsequently, we replicated the procedure from Step 1 using this newly trained model. This iteration resulted in the generation of over 500 billion synthetic tokens, a process detailed in Step 2. 2. The final result of this process is morning 500 bilions of high quality Vietnamese tokens ready to be used to continue pretrain on the base LLaMA 2 with the expanded tokenizer. 3. 2 Supervised Instruction Fine-tuning \\nIn collaboration with Nous Research 3, we employed proprietary methodologies to create 500,000 Viet- namese synthetic samples. These samples encompassed both instructional and conversational formats. To enhance the dataset, an additional 500,000 English samples were sourced from the OpenHermes-2. 5 [tek23] and Capybara [DS23] datasets, also provided by Nous Research. This integration of datasets culminated in a comprehensive collection of 1 million samples, paving the way for the development of a bilingual language model. The comprehensive final dataset encompasses an array of tasks, including reasoning, role-playing, poem writing, coding, function calling, and agent prompting.\\nContext 6: This diversity ensures a broad scope of capabilities in the final model. We conducted a full fine-tuning of our pre- trained model over four epochs, which led to the creation of a state-of-the-art foundation language model specifically tailored for the Vietnamese language, more on the result in Section 4. 3. 3 VinaLLaMA-2. 7B \\nAdopting the structured pruning procedure outlined in [XGZC23], we successfully reduced our model to a more compact variant with 2. 7 billion parameters. This process involved strategically pruning the network while retaining its core functional capabilities. Following the reduction in size, we applied the same supervised fine-tuning methodology to this smaller variant as was used for the 7B model, ensuring consistency in training approach across different model scales. 4 Evaluation \\nIn our research, we utilized three distinct evaluation benchmarks: VLSP, VMLU, and the Vicuna Benchmark, with the latter being translated into Vietnamese by VinAI Research. For each of these benchmarks, we implemented a dual-evaluation approach. Specifically, we conducted separate exper- iments for two categories of models: those in their pre-trained state and those that had undergone instructional fine-tuning. This methodology allowed us to compare the baseline capabilities of the pre-trained models against their performance post-instructional fine-tuning, providing insights into the effectiveness of fine-tuning in enhancing model proficiency within the context of these Vietnamese language benchmarks. For the pre-training benchmarks, we selected VLSP’s hoa-7b, BLOOM-7B, and PhoGPT-7B5 as the base models. In contrast, the instructional fine-tuned benchmarks comprised BLOOMZ-7B, Vietcuna- 7B-v3, PhoGPT-7B5-Instruct, URA-LLaMA-13B and SeaLLM-7B-chat. This diverse set of models provided a comprehensive perspective, enabling us to assess both the inherent capabilities of these models in their original form and their enhanced performance post-fine-tuning, specifically tailored to the Vietnamese language benchmarks.\\nContext 7: 4. 1 VLSP \\nThe first benchmark employed is the VLSP-LLM 2023 [CHC+23]. This benchmark parallels the Open- LLM Leaderboard by HuggingFace, tailored specifically for the Vietnamese language. It encompasses four distinct assessments: ARC Challenge, HellaSwag, MMLU, and TruthfulQA, each meticulously adapted to Vietnamese. This comprehensive benchmark suite allows for a robust evaluation of lan- guage models in understanding and generating Vietnamese text across various domains and complex- ities. The results are reported in Table 1 and Table 2. In the pretrained segment of our study, the VinaLLaMA-7B model demonstrated superior performance compared to other open-source Large Lan- guage Models (LLMs) that support Vietnamese. This was evident in its outperformance on three of the four benchmarks, leading to it achieving state-of-the-art results based on the average score across these benchmarks. This signifies the robustness and effectiveness of the VinaLLaMA-7B model in handling a variety of tasks in the Vietnamese language context. In the supervised fine-tuning domain, our results continued to reflect a high level of excellence. No- tably, VinaLLaMA-7B-chat, comparable in scale, astonishingly outperformed larger models, including those with 13 billion parameters, in terms of average score. This impressive achievement highlights the efficacy of the fine-tuning process and the model’s adeptness at leveraging its training to deliver outstanding performance, even against much larger counterparts. Adding to this, the VinaLLaMA- 2. 7B, a significantly smaller model, showcased remarkable performance. It not only competed closely with larger 7B variants but also exceeded PhoGPT-7B5-Instruct in performance, while achieving in- ferencing speeds 60% faster.\\nContext 8: This showcases the model’s optimized balance between size, speed, and performance. The success of VinaLLaMA-7B-chat, and the noteworthy performance of VinaLLaMA- 2. 7B, underscore the considerable potential of well-implemented fine-tuning strategies with synthetic data in elevating the capabilities of Large Language Models, especially in contexts requiring specialized language processing. 4. 2 VMLU \\nVMLU [AoST23], a benchmark suite tailored for evaluating foundation models in the Vietnamese language, comprises 10,880 multiple-choice questions across 58 subjects in domains like STEM, Hu- manities, and Social Sciences. Its diverse range of difficulty levels tests models from basic knowledge to advanced problem-solving. We selected VMLU for benchmarking due to its comprehensive coverage of Vietnam-related questions, providing a relevant and challenging environment for assessing model ca- pabilities in a context-specific manner. We conducted our experiments on the validation set of VMLU since the answers to test set are not publicly available, URA-LLaMA-13B is also not being test due to the lack of availability at testing time. We reported the results under two few-shot settings: 0-shot and 5-shot, which can be viewed in Table 3 and 4. 4. 3 Vicuna Benchmark Vietnamese \\nThe Vicuna Benchmark [ZCS+23], translated into Vietnamese by VinAI, serves as our final bench- mark. This comprehensive benchmark is composed of 80 distinct instructions spanning 9 diverse areas, providing a broad spectrum for assessing model capabilities. Uniquely, the evaluation of the results from all participating models is conducted using GPT-4, which introduces an innovative approach to performance assessment. This methodology employs an ELO ranking system, traditionally used in chess and other competitive games, to rate the models. Such a system offers a dynamic and rela- tive measure of model performance, allowing for a nuanced and comparative analysis of each model’s proficiency in handling a variety of tasks and instructions within the benchmark.\\nContext 9: This ELO-based evaluation provides a clear and quantifiable ranking of the models, reflecting their effectiveness and adaptability in the context of the Vietnamese language and the specific challenges presented by the Vicuna Benchmark. In our Vicuna Benchmark evaluation, responses from models were assessed using a detailed five- point scale: 0 (’very bad’), 1 (’bad’), 2 (’ok’), 3 (’good’), and 4 (’very good’). This granular scoring system allows for an in-depth evaluation of the quality of each model’s response. The final ELO score for each model is computed by aggregating these individual ratings, providing a holistic measure of a model’s overall performance across the benchmark’s varied tasks. To ensure language relevance, we implemented a strict rule: any response not in Vietnamese is automatically assigned a score of 0. This criterion underscores the importance of language-specific accuracy in our evaluation. The collective results, reflecting model performances across different tasks, are visually represented in Figure 4. In the interest of transparency and further research, we have made VinaLLaMA’s responses and our evaluation code publicly available 4 5. This not only allows for independent verification of our results but also facilitates further advancements in the field by providing valuable resources to other researchers. The benchmark results revealed that VinaLLaMA showcased commendable performance in Viet- namese, closely trailing behind ChatGPT-3. 5-Turbo in some benchmarks. This indicates that VinaLLaMA is highly effective in Vietnamese language tasks, with only a slight margin separating it from the more advanced ChatGPT-3. 5-Turbo, as shown in Table 5 and Table 6. PhoGPT also demonstrated better outcomes compared to other Vietnamese benchmarks, yet it still fell significantly behind in some areas, highlighting potential avenues for improvement. In contrast, URA-LLaMA-7B and 13B models performed less effectively, with some of their re- sponses not aligning with Vietnamese language requirements.\\nContext 10: This points to a critical need for targeted enhancements or language-specific training for these models to better cater to Vietnamese language processing. 5 Acknowledgement \\nWe are also immensely grateful to the teams at Nous Research, LAION, FPT Software AI Center, and Symato Team. Their assistance in the early stages of VinaLLaMA’s evaluation was crucial, providing insights that guided further improvements and enhancements of the model. We also want to send our appreciation to Nam Pham and Nhan Nguyen for the contribution in our public dataset. Lastly, our profound appreciation is extended to Google Cloud and Stability. AI. Their generous computational support was a cornerstone in bringing VinaLLaMA to fruition, enabling the intensive training and development processes required for such a sophisticated large language model. Their contribution was vital in transforming our vision for VinaLLaMA into a reality. 6 Conclusion \\nIn conclusion, the development of VinaLLaMA marks a significant milestone in the realm of language models, particularly in the context of Vietnamese language processing. Achieving state-of-the-art (SOTA) scores across all Vietnamese benchmarks, VinaLLaMA has demonstrated exceptional profi- ciency and adaptability. While its performance in English benchmarks was slightly less dominant, it still showed considerable competence, underscoring its effectiveness as a bilingual model. A key factor in VinaLLaMA’s success is the strategic use of carefully crafted synthetic data in its training regimen. This approach has proven to be highly effective, yielding impressive results and highlighting the importance of diverse and well-designed training datasets in the development of robust language models. VinaLLaMA’s achievements not only set a new standard for language models in Vietnamese but also contribute valuable insights into the broader field of natural language processing. It exemplifies how meticulous data preparation and comprehensive training can significantly enhance the capabilities of language models, paving the way for future advancements in this dynamic and rapidly evolving domain.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So sánh câu trả lời"
      ],
      "metadata": {
        "id": "v0ydkFwqTGXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "llm = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "YlkLM4WIS8D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = build_final_context(chunks)\n",
        "message = [{\"role\": \"user\", \"content\": question.format(context=context)}]\n",
        "\n",
        "response = llm.chat.completions.create(model='gpt-3.5-turbo',\n",
        "      messages= message,\n",
        "      temperature=0.1,\n",
        "      max_tokens=4096)"
      ],
      "metadata": {
        "id": "FnVPIcVaTdeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "twsBwN7AUCSq",
        "outputId": "7eb341a3-202c-4ffa-82c5-23ce8cf5c2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"VinaLLaMA is an open-source Large Language Model for the Vietnamese language, built upon LLaMA-2 with an additional 800 billion trained tokens. It demonstrates fluency in Vietnamese and understanding of Vietnamese culture. VinaLLaMA-7B-chat achieves SOTA results on key benchmarks, marking a significant advancement in the Vietnamese AI landscape. The model is accessible on HuggingFace, contributing to global AI research and enhancing Vietnamese language processing. The training procedure involves pretraining on public and synthetic data, supervised instruction fine-tuning, and model evaluation on VLSP, VMLU, and Vicuna Benchmark Vietnamese. VinaLLaMA showcases commendable performance in Vietnamese language tasks, with potential for further enhancements. The model's success is attributed to strategic use of synthetic data in training, highlighting the importance of diverse and well-designed datasets in developing robust language models.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "context=build_final_context(chunks)"
      ],
      "metadata": {
        "id": "vnY-zwnVUUH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_chunks = []\n",
        "for chunk in chunks:\n",
        "  message = [{\"role\": \"user\", \"content\": summarize_prompt.format(context=chunk)}]\n",
        "  response = llm.chat.completions.create(model='gpt-3.5-turbo',\n",
        "        messages= message,\n",
        "        temperature=0.1,\n",
        "        max_tokens=4096)\n",
        "  summarize_chunks.append(response.choices[0].message.content)\n",
        "\n",
        "conext = build_final_context(summarize_chunks)\n",
        "message = [{\"role\": \"user\", \"content\": final_ans_prompt.format(context=context)}]\n",
        "\n",
        "final_response = llm.chat.completions.create(model='gpt-3.5-turbo',\n",
        "      messages= message,\n",
        "      temperature=0.1,\n",
        "      max_tokens=4096)\n",
        "\n"
      ],
      "metadata": {
        "id": "q_ziUYhvUFps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "YE-99WJMUVER",
        "outputId": "7c04a904-de1a-458d-ae4a-09f9ff39e5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'VinaLLaMA là một Large Language Model tiên tiến cho ngôn ngữ Việt Nam, được xây dựng trên nền tảng LLaMA-2 với 800 tỷ token được huấn luyện thêm. Model này không chỉ thể hiện sự trôi chảy trong tiếng Việt mà còn hiểu sâu về văn hóa Việt Nam. VinaLLaMA-7B-chat đạt kết quả SOTA trên các bài kiểm tra quan trọng, đánh dấu một bước tiến quan trọng trong lĩnh vực AI tiếng Việt. Qua việc sử dụng dữ liệu tổng hợp chất lượng cao, model này đã chứng minh khả năng vượt trội và đa dạng ứng dụng trong nghiên cứu NLP.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Refinement"
      ],
      "metadata": {
        "id": "tgKuwtLnV-rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples_retrieval = ['Hôm nay, giá Bitcoin đã đạt đến một đỉnh mới.',\n",
        "           'Phở Việt Nam rất ngon, Hà Nội nổi tiếng với món Phở Bò.',\n",
        "           'Phở là món ăn Việt Nam xuất hiện ở rất nhiều nơi trên thế giới.',\n",
        "           'Một thị trường khó như Nhật cũng rất ấn tượng với món Phở Bò Việt Nam.']\n",
        "\n",
        "context = build_final_context(samples_retrieval)\n",
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcTuejv_V_Wj",
        "outputId": "20dad9f9-e51b-4e7e-c579-308b833e1b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1: Hôm nay, giá Bitcoin đã đạt đến một đỉnh mới.\n",
            "Context 2: Phở Việt Nam rất ngon, Hà Nội nổi tiếng với món Phở Bò.\n",
            "Context 3: Phở là món ăn Việt Nam xuất hiện ở rất nhiều nơi trên thế giới.\n",
            "Context 4: Một thị trường khó như Nhật cũng rất ấn tượng với món Phở Bò Việt Nam.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_question = \"Hãy giới thiệu về món Phở Bò Việt Nam\"\n",
        "refinement_prompt = \"\"\"\n",
        "Dưới đây là các nội dung được trích xuất từ Database:\n",
        "---CONTEXT---\n",
        "{context}\n",
        "---END CONTEXT---\n",
        "Nhiệm vụ của bạn là hãy viết lại những nội dung trên dựa trên câu hỏi dưới đây, hãy loại bỏ các nội dung không liên quan để tránh gây nhiễu loạn. Tôi chỉ cần phần viết lại của bạn, vui lòng không thêm bất kì thông tin nào khác.\n",
        "Câu hỏi: {question}\n",
        "\"\"\"\n",
        "\n",
        "sample_message = [{\"role\": \"user\", \"content\": refinement_prompt.format(context=context, question=sample_question)}]\n",
        "\n",
        "sample_response = llm.chat.completions.create(model='gpt-3.5-turbo',\n",
        "      messages=sample_message,\n",
        "      temperature=0.1,\n",
        "      max_tokens=4096)"
      ],
      "metadata": {
        "id": "HcmDd1piWt8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMoIjOdLXUuX",
        "outputId": "a7ea055f-4759-4521-bf94-d1bc0bb756cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phở Bò Việt Nam là một món ăn ngon và nổi tiếng của Việt Nam, được biết đến với hương vị đậm đà và thơm ngon. Món ăn này xuất hiện ở nhiều nơi trên thế giới và được đánh giá cao, ngay cả ở những thị trường khó tính như Nhật Bản.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_prompt = \"\"\"\n",
        "Dưới đây là các nội dung được trích xuất từ Database:\n",
        "---CONTEXT---\n",
        "{context}\n",
        "---END CONTEXT---\n",
        "Nhiệm vụ của bạn là trả lời câu hỏi dựa trên các nội dung trên. Tôi chỉ cần trả lời câu hỏi, vui lòng không thêm bất kì thông tin nào khác.\n",
        "Nếu thư thông tin đưa cho không liên quan đến câu hỏi, vui lòng trả lời \"Tôi không biết\"\n",
        "Câu hỏi: {question}\n",
        "Trả lời:\n",
        "\"\"\"\n",
        "\n",
        "step2_message = [{\"role\": \"user\", \"content\": answer_prompt.format(context=context, question=sample_question)}]\n",
        "\n",
        "step2_response = llm.chat.completions.create(model='gpt-3.5-turbo',\n",
        "      messages=step2_message,\n",
        "      temperature=0.1,\n",
        "      max_tokens=4096)\n",
        "\n",
        "print(step2_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntiU_KgDXbcX",
        "outputId": "d5e9e3ef-fe17-4c53-a54a-8651f3799919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phở Bò là một món ăn truyền thống của Việt Nam, nổi tiếng với hương vị đậm đà và thơm ngon. Món ăn này được làm từ nước dùng phở, thịt bò, bún, và các loại gia vị như hành, gừng, và hồ tiêu. Phở Bò thường được ăn kèm với rau sống, giá, và chanh để tạo thêm hương vị đặc trưng. Món Phở Bò đã trở thành biểu tượng của ẩm thực Việt Nam và được yêu thích trên toàn thế giới.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJd_xQW2X2Gx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}