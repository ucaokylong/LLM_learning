{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucaokylong/LLM_learning/blob/main/function_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivHdTOyFH0nT"
      },
      "source": [
        "# OpenAI Function Calling 101\n",
        "M·ªôt trong nh·ªØng kh√≥ khƒÉn khi s·ª≠ d·ª•ng c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) nh∆∞ ChatGPT l√† ch√∫ng kh√¥ng t·∫°o ra ƒë·∫ßu ra d·ªØ li·ªáu c√≥ c·∫•u tr√∫c. ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªëi v·ªõi c√°c h·ªá th·ªëng l·∫≠p tr√¨nh ph·ª• thu·ªôc ph·∫ßn l·ªõn v√†o d·ªØ li·ªáu c√≥ c·∫•u tr√∫c ƒë·ªÉ t∆∞∆°ng t√°c. V√≠ d·ª•, n·∫øu b·∫°n mu·ªën x√¢y d·ª±ng m·ªôt ch∆∞∆°ng tr√¨nh ph√¢n t√≠ch c·∫£m x√∫c c·ªßa m·ªôt b√†i ƒë√°nh gi√° phim, b·∫°n c√≥ th·ªÉ ph·∫£i th·ª±c hi·ªán m·ªôt ƒëo·∫°n code tr√¥ng gi·ªëng nh∆∞ sau:\n",
        "\n",
        "```\n",
        "prompt = f'''\n",
        "Please perform a sentiment analysis on the following movie review:\n",
        "{MOVIE_REVIEW_TEXT}\n",
        "Please output your response as a single word: either \"Positive\" or \"Negative\". Do not add any extra characters.\n",
        "'''\n",
        "```\n",
        "\n",
        "V·∫•n ƒë·ªÅ l√† ƒëi·ªÅu n√†y kh√¥ng ph·∫£i l√∫c n√†o c≈©ng hi·ªáu qu·∫£. Kh√° ph·ªï bi·∫øn l√† LLM s·∫Ω th√™m v√†o m·ªôt d·∫•u ch·∫•m kh√¥ng mong mu·ªën ho·∫∑c gi·∫£i th√≠ch d√†i h∆°n nh∆∞: \"C·∫£m x√∫c c·ªßa b·ªô phim n√†y l√†: T√≠ch c·ª±c.\" M·∫∑c d√π b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy (regex) ƒë·ªÉ l·∫•y ra c√¢u tr·∫£ l·ªùi (ü§¢), nh∆∞ng r√µ r√†ng ƒë√¢y kh√¥ng ph·∫£i l√† l√Ω t∆∞·ªüng. ƒêi·ªÅu l√Ω t∆∞·ªüng s·∫Ω l√† n·∫øu LLM tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng c·∫•u tr√∫c JSON nh∆∞ sau:\n",
        "\n",
        "```\n",
        "{\n",
        "    'sentiment': 'positive'\n",
        "}\n",
        "```\n",
        "\n",
        "OpenAI ƒë√£ gi·ªõi thi·ªáu m·ªôt t√≠nh nƒÉng m·ªõi g·ªçi l√† function calling, gi√∫p gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ tr√™n. Function calling ch√≠nh l√† c√¢u tr·∫£ l·ªùi cho v·∫•n ƒë·ªÅ tr√™n. Jupyter notebook n√†y s·∫Ω minh h·ªça m·ªôt v√≠ d·ª• ƒë∆°n gi·∫£n v·ªÅ c√°ch s·ª≠ d·ª•ng function calling m·ªõi c·ªßa OpenAI trong Python. N·∫øu b·∫°n mu·ªën xem t√†i li·ªáu ƒë·∫ßy ƒë·ªß, [vui l√≤ng xem li√™n k·∫øt n√†y](https://platform.openai.com/docs/guides/gpt/function-calling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT1ePnTaH0nV"
      },
      "source": [
        "## Notebook Setup\n",
        "H√£y b·∫Øt ƒë·∫ßu v·ªõi c√°c import. B√¢y gi·ªù, c√≥ th·ªÉ b·∫°n ƒë√£ c√†i ƒë·∫∑t client Python `openai`, nh∆∞ng r·∫•t c√≥ th·ªÉ b·∫°n c·∫ßn n√¢ng c·∫•p n√≥ ƒë·ªÉ c√≥ ƒë∆∞·ª£c ch·ª©c nƒÉng function calling m·ªõi. ƒê√¢y l√† c√°ch n√¢ng c·∫•p trong Terminal / Powershell c·ªßa b·∫°n b·∫±ng `pip`:\n",
        "\n",
        "```\n",
        "pip install openai --upgrade\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --upgrade"
      ],
      "metadata": {
        "id": "iN4qCK21hBor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzJqI_a6H0nV"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary Python libraries\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URyKfrF1H0nW"
      },
      "source": [
        "ƒê·ªÉ ki·ªÉm tra ch·ª©c nƒÉng function calling, t√¥i ƒë√£ vi·∫øt m·ªôt ƒëo·∫°n \"About Me\" ng·∫Øn ch·ª©a c√°c s·ª± th·∫≠t c·ª• th·ªÉ m√† ch√∫ng ta s·∫Ω ph√¢n t√≠ch th√†nh c√°c c·∫•u tr√∫c d·ªØ li·ªáu ph√π h·ª£p, bao g·ªìm s·ªë nguy√™n v√† chu·ªói. H√£y t·∫£i vƒÉn b·∫£n n√†y v√†o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHNSQ593H0nW"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Loading the \"About Me\" text from local file\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "about_me = \"Hello! My name is David Hundley. I am a principal machine learning engineer at State Farm. I enjoy learning about AI and teaching what I learn back to others. I have two daughters. I drive a Tesla Model 3, and my favorite video game series is The Legend of Zelda.\"\n",
        "\n",
        "print(about_me)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W4Q38BcH0nX"
      },
      "source": [
        "## The Pre-Function Calling Days\n",
        "Tr∆∞·ªõc khi ch√∫ng ta minh h·ªça function calling, h√£y minh h·ªça c√°ch ch√∫ng ta ƒë√£ s·ª≠ d·ª•ng prompt engineering v√† Regex ƒë·ªÉ t·∫°o ra m·ªôt JSON c√≥ c·∫•u tr√∫c m√† ch√∫ng ta c√≥ th·ªÉ l√†m vi·ªác m·ªôt c√°ch l·∫≠p tr√¨nh sau n√†y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7u7WOlAH0nX"
      },
      "outputs": [],
      "source": [
        "# Engineering a prompt to extract as much information from \"About Me\" as a JSON object\n",
        "about_me_prompt = f'''\n",
        "Please extract information as a JSON object. Please look for the following pieces of information.\n",
        "Name\n",
        "Job title\n",
        "Company\n",
        "Number of children as a single number\n",
        "Car make\n",
        "Car model\n",
        "Favorite video game series\n",
        "\n",
        "This is the body of text to extract the information from:\n",
        "{about_me}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ276CVDH0nX"
      },
      "outputs": [],
      "source": [
        "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "openai_response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': about_me_prompt}]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXT-v6-bH0nX",
        "outputId": "9fbad469-04e0-4ffd-d988-e6efd6b1370b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Name': 'David Hundley', 'Job title': 'Principal Machine Learning Engineer', 'Company': 'State Farm', 'Number of children': 2, 'Car make': 'Tesla', 'Car model': 'Model 3', 'Favorite video game series': 'The Legend of Zelda'}\n"
          ]
        }
      ],
      "source": [
        "# Loading the response as a JSON object\n",
        "json_response = json.loads(openai_response.choices[0].message.content)\n",
        "print(json_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjGXw7QH0nX"
      },
      "source": [
        "## Using the New Function Calling Capabilities\n",
        "B√¢y gi·ªù ch√∫ng ta ƒë√£ minh h·ªça c√°ch ch√∫ng ta ƒë√£ t·ª´ng l·∫•y ƒë∆∞·ª£c JSON c√≥ c·∫•u tr√∫c trong nh·ªØng ng√†y tr∆∞·ªõc khi c√≥ function calling, h√£y chuy·ªÉn sang c√°ch ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng function calling ƒë·ªÉ tr√≠ch xu·∫•t c√°c k·∫øt qu·∫£ t∆∞∆°ng t·ª± nh∆∞ng theo c√°ch nh·∫•t qu√°n h∆°n cho vi·ªác s·ª≠ d·ª•ng h·ªá th·ªëng c·ªßa ch√∫ng ta. Ch√∫ng ta s·∫Ω b·∫Øt ƒë·∫ßu ƒë∆°n gi·∫£n h∆°n v·ªõi m·ªôt h√†m t√πy ch·ªânh duy nh·∫•t v√† sau ƒë√≥ gi·∫£i quy·∫øt m·ªôt s·ªë ch·ª©c nƒÉng \"n√¢ng cao\" h∆°n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWikJtaqH0nX"
      },
      "outputs": [],
      "source": [
        "# Defining our initial extract_person_info function\n",
        "def extract_person_info(name, job_title, num_children):\n",
        "    '''\n",
        "    Prints basic \"About Me\" information\n",
        "\n",
        "    Inputs:\n",
        "        name (str): Name of the person\n",
        "        job_title (str): Job title of the person\n",
        "        num_chilren (int): The number of children the parent has.\n",
        "    '''\n",
        "\n",
        "    print(f'This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNS7-6XqH0nX"
      },
      "outputs": [],
      "source": [
        "# Defining how we want ChatGPT to call our custom functions\n",
        "my_custom_functions = [\n",
        "    {\n",
        "        'name': 'extract_person_info',\n",
        "        'description': 'Get \"About Me\" information from the body of the input text',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person'\n",
        "                },\n",
        "                'job_title': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Job title of the person'\n",
        "                },\n",
        "                'num_children': {\n",
        "                    'type': 'integer',\n",
        "                    'description': 'Number of children the person is a parent to'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVKEN9tQH0nY",
        "outputId": "3d8585a6-035f-49db-dd0b-b8ddd6fd8db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-94o26OACU2zjAMUwNvIDsXvqnRCQn', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1710932666, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=30, prompt_tokens=147, total_tokens=177))\n"
          ]
        }
      ],
      "source": [
        "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "openai_response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': about_me}],\n",
        "    functions = my_custom_functions,\n",
        "    function_call = 'auto'\n",
        ")\n",
        "\n",
        "print(openai_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai_response.choices[0].message.function_call)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceI9oBI7iQWE",
        "outputId": "dfe7a6bc-bba6-467d-c587-5cb88cf2d7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7kQ2GsUH0nY"
      },
      "source": [
        "### What if the prompt I submit doesn't contain the information I want to extract per my custom function?\n",
        "Copy code\n",
        "Trong v√≠ d·ª• ban ƒë·∫ßu c·ªßa ch√∫ng ta, h√†m t√πy ch·ªânh ƒë√£ t√¨m c√°ch tr√≠ch xu·∫•t ba th√¥ng tin r·∫•t c·ª• th·ªÉ v√† ch√∫ng ta ƒë√£ ch·ª©ng minh r·∫±ng ƒëi·ªÅu n√†y ho·∫°t ƒë·ªông th√†nh c√¥ng b·∫±ng c√°ch truy·ªÅn vƒÉn b·∫£n \"About Me\" t√πy ch·ªânh c·ªßa t√¥i d∆∞·ªõi d·∫°ng m·ªôt prompt. Nh∆∞ng b·∫°n c√≥ th·ªÉ t·ª± h·ªèi, ƒëi·ªÅu g√¨ s·∫Ω x·∫£y ra n·∫øu b·∫°n truy·ªÅn v√†o b·∫•t k·ª≥ prompt n√†o kh√°c kh√¥ng ch·ª©a th√¥ng tin ƒë√≥?\n",
        "\n",
        "H√£y nh·ªõ l·∫°i r·∫±ng ch√∫ng ta ƒë√£ ƒë·∫∑t m·ªôt tham s·ªë trong l·ªánh g·ªçi API client c·ªßa m√¨nh g·ªçi l√† function_call m√† ch√∫ng ta ƒë·∫∑t th√†nh auto. Ch√∫ng ta s·∫Ω kh√°m ph√° ƒëi·ªÅu n√†y s√¢u h∆°n n·ªØa trong ph·∫ßn ti·∫øp theo, nh∆∞ng v·ªÅ c∆° b·∫£n, tham s·ªë n√†y ƒëang y√™u c·∫ßu ChatGPT s·ª≠ d·ª•ng ph√°n ƒëo√°n t·ªët nh·∫•t c·ªßa n√≥ ƒë·ªÉ t√¨m ra khi n√†o c·∫ßn c·∫•u tr√∫c ƒë·∫ßu ra cho m·ªôt trong c√°c h√†m t√πy ch·ªânh c·ªßa ch√∫ng ta.\n",
        "\n",
        "V·∫≠y ƒëi·ªÅu g√¨ s·∫Ω x·∫£y ra khi ch√∫ng ta g·ª≠i m·ªôt prompt kh√¥ng kh·ªõp v·ªõi b·∫•t k·ª≥ h√†m t√πy ch·ªânh n√†o c·ªßa ch√∫ng ta? N√≥i m·ªôt c√°ch ƒë∆°n gi·∫£n, n√≥ s·∫Ω m·∫∑c ƒë·ªãnh tr·ªü l·∫°i h√†nh vi th√¥ng th∆∞·ªùng nh∆∞ th·ªÉ function calling kh√¥ng t·ªìn t·∫°i. H√£y ki·ªÉm tra ƒëi·ªÅu n√†y v·ªõi m·ªôt prompt t√πy √Ω: \"Th√°p Eiffel cao bao nhi√™u?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1UBGLPMH0nY",
        "outputId": "7abf4457-cc06-4668-9f62-6a1cea4c58b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-94o7fPVQErjmsLTu7ynKXW3BQNxdG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Eiffel Tower is 1,063 feet (324 meters) tall, including antennas.', role='assistant', function_call=None, tool_calls=None))], created=1710933011, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=21, prompt_tokens=97, total_tokens=118))\n"
          ]
        }
      ],
      "source": [
        "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "openai_response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': 'How tall is the Eiffel Tower?'}],\n",
        "    functions = my_custom_functions,\n",
        "    function_call = 'auto'\n",
        ")\n",
        "\n",
        "print(openai_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-BK6U8SH0nY"
      },
      "outputs": [],
      "source": [
        "# Defining a function to extract only vehicle information\n",
        "def extract_vehicle_info(vehicle_make, vehicle_model):\n",
        "    '''\n",
        "    Prints basic vehicle information\n",
        "\n",
        "    Inputs:\n",
        "        - vehicle_make (str): Make of the vehicle\n",
        "        - vehicle_model (str): Model of the vehicle\n",
        "    '''\n",
        "\n",
        "    print(f'Vehicle make: {vehicle_make}\\nVehicle model: {vehicle_model}')\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function to extract all information provided in the original \"About Me\" prompt\n",
        "def extract_all_info(name, job_title, num_children, vehicle_make, vehicle_model, company_name, favorite_vg_series):\n",
        "    '''\n",
        "    Prints the full \"About Me\" information\n",
        "\n",
        "    Inputs:\n",
        "        - name (str): Name of the person\n",
        "        - job_title (str): Job title of the person\n",
        "        - num_chilren (int): The number of children the parent has\n",
        "        - vehicle_make (str): Make of the vehicle\n",
        "        - vehicle_model (str): Model of the vehicle\n",
        "        - company_name (str): Name of the company the person works for\n",
        "        - favorite_vg_series (str): Person's favorite video game series.\n",
        "    '''\n",
        "\n",
        "    print(f'''\n",
        "    This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.\n",
        "    They drive a {vehicle_make} {vehicle_model}.\n",
        "    They work for {company_name}.\n",
        "    Their favorite video game series is {favorite_vg_series}.\n",
        "    ''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MCTQFNfH0nY"
      },
      "outputs": [],
      "source": [
        "# Defining how we want ChatGPT to call our custom functions\n",
        "my_custom_functions = [\n",
        "    {\n",
        "        'name': 'extract_person_info',\n",
        "        'description': 'Get \"About Me\" information from the body of the input text',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person'\n",
        "                },\n",
        "                'job_title': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Job title of the person'\n",
        "                },\n",
        "                'num_children': {\n",
        "                    'type': 'integer',\n",
        "                    'description': 'Number of children the person is a parent to'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'extract_vehicle_info',\n",
        "        'description': 'Extract the make and model of the person\\'s car',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'vehicle_make': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Make of the person\\'s vehicle'\n",
        "                },\n",
        "                'vehicle_model': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Model of the person\\'s vehicle'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'extract_all_info',\n",
        "        'description': 'Extract all information about a person including their vehicle make and model',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person'\n",
        "                },\n",
        "                'job_title': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Job title of the person'\n",
        "                },\n",
        "                'num_children': {\n",
        "                    'type': 'integer',\n",
        "                    'description': 'Number of children the person is a parent to'\n",
        "                },\n",
        "                'vehicle_make': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Make of the person\\'s vehicle'\n",
        "                },\n",
        "                'vehicle_model': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Model of the person\\'s vehicle'\n",
        "                },\n",
        "                'company_name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the company the person works for'\n",
        "                },\n",
        "                'favorite_vg_series': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person\\'s favorite video game series'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7bKZqEnH0nY"
      },
      "source": [
        "Now let's demonstrate what happens when we apply 3 different samples to all of the custom functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ondbhtOmH0nY"
      },
      "outputs": [],
      "source": [
        "# Defining a list of samples\n",
        "samples = [\n",
        "    about_me,\n",
        "    'My name is David Hundley. I am a principal machine learning engineer, and I have two daughters.',\n",
        "    'She drives a Kia Sportage.'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNF0ss92H0nY",
        "outputId": "08dea244-f134-4886-8a5d-6dfe94574b77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample #1's results:\n",
            "ChatCompletion(id='chatcmpl-94o8LMn34OQ4Nw6nG7Aq12ehlMhjG', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2,\"vehicle_make\":\"Tesla\",\"vehicle_model\":\"Model 3\",\"company_name\":\"State Farm\",\"favorite_vg_series\":\"The Legend of Zelda\"}', name='extract_all_info'), tool_calls=None))], created=1710933053, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=58, prompt_tokens=320, total_tokens=378))\n",
            "Sample #2's results:\n",
            "ChatCompletion(id='chatcmpl-94o8NGEFk16kB8YZvuJpk3c5cl5JN', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1710933055, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=30, prompt_tokens=282, total_tokens=312))\n",
            "Sample #3's results:\n",
            "ChatCompletion(id='chatcmpl-94o8OzN5cpUhTNZ2dplMqMyZma0uu', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"vehicle_make\":\"Kia\",\"vehicle_model\":\"Sportage\"}', name='extract_vehicle_info'), tool_calls=None))], created=1710933056, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=23, prompt_tokens=268, total_tokens=291))\n"
          ]
        }
      ],
      "source": [
        "# Iterating over the three samples\n",
        "for i, sample in enumerate(samples):\n",
        "\n",
        "    print(f'Sample #{i + 1}\\'s results:')\n",
        "\n",
        "    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "    openai_response = openai.chat.completions.create(\n",
        "        model = 'gpt-3.5-turbo',\n",
        "        messages = [{'role': 'user', 'content': sample}],\n",
        "        functions = my_custom_functions,\n",
        "        function_call = 'auto'\n",
        "    )\n",
        "\n",
        "    # Printing the sample's response\n",
        "    print(openai_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD7qtImGH0nY"
      },
      "source": [
        "With each of the respective prompts, ChatGPT selected the correct custom function, and we can specifically note that in the `name` value under `function_call` in the API's response object. In addition to this being a handy way to identify which function to use the arguments for, we can programmatically map our actual custom Python function to this value to run the correct code appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGfDFvrTH0nZ",
        "outputId": "fd7e6f53-2bcb-45ba-d8dd-0847933bf1a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample #1's results:\n",
            "\n",
            "    This person's name is David Hundley. Their job title is Principal Machine Learning Engineer, and they have 2 children.\n",
            "    They drive a Tesla Model 3.\n",
            "    They work for State Farm.\n",
            "    Their favorite video game series is The Legend of Zelda.\n",
            "    \n",
            "Sample #2's results:\n",
            "This person's name is David Hundley. Their job title is Principal Machine Learning Engineer, and they have 2 children.\n",
            "Sample #3's results:\n",
            "Vehicle make: Kia\n",
            "Vehicle model: Sportage\n"
          ]
        }
      ],
      "source": [
        "# Iterating over the three samples\n",
        "for i, sample in enumerate(samples):\n",
        "\n",
        "    print(f'Sample #{i + 1}\\'s results:')\n",
        "\n",
        "    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "    openai_response = openai.chat.completions.create(\n",
        "        model = 'gpt-3.5-turbo',\n",
        "        messages = [{'role': 'user', 'content': sample}],\n",
        "        functions = my_custom_functions,\n",
        "        function_call = 'auto'\n",
        "    ).choices[0].message\n",
        "\n",
        "    # Checking to see that a function call was invoked\n",
        "    if openai_response.function_call:\n",
        "        # Checking to see which specific function call was invoked\n",
        "        function_called = openai_response.function_call.name\n",
        "\n",
        "        # Extracting the arguments of the function call\n",
        "        function_args = json.loads(openai_response.function_call.arguments)\n",
        "\n",
        "        # Invoking the proper functions\n",
        "        if function_called == 'extract_person_info':\n",
        "            extract_person_info(*list(function_args.values()))\n",
        "        elif function_called == 'extract_vehicle_info':\n",
        "            extract_vehicle_info(*list(function_args.values()))\n",
        "        elif function_called == 'extract_all_info':\n",
        "            extract_all_info(*list(function_args.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpXnf2mAH0nZ"
      },
      "source": [
        "## OpenAI Function Calling with LangChain\n",
        "Given its popularity amongst the Generative AI community, I thought I'd re-visit this notebook and add some code to show how you might make use of this exact same functionality in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhOmubQskrv6",
        "outputId": "5b0962ac-edd0-4581-9b50-6d40ada19a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.28 (from langchain)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.14.2)\n",
            "Collecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.31->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, tiktoken, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-openai-0.0.8 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zivmk4LtH0nZ"
      },
      "outputs": [],
      "source": [
        "# Importing the LangChain objects\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain.chains.openai_functions import create_structured_output_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp067ipkH0nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f1f564-6648-4ec7-bfa9-4c90fb0567f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Setting the proper instance of the OpenAI model\n",
        "llm = ChatOpenAI(model = 'gpt-3.5-turbo-0613')\n",
        "\n",
        "# Setting a LangChain ChatPromptTemplate\n",
        "chat_prompt_template = ChatPromptTemplate.from_template('{my_prompt}')\n",
        "\n",
        "# Setting the JSON schema for extracting vehicle information\n",
        "langchain_json_schema = {\n",
        "    'name': 'extract_vehicle_info',\n",
        "    'description': 'Extract the make and model of the person\\'s car',\n",
        "    'type': 'object',\n",
        "    'properties': {\n",
        "        'vehicle_make': {\n",
        "            'title': 'Vehicle Make',\n",
        "            'type': 'string',\n",
        "            'description': 'Make of the person\\'s vehicle'\n",
        "        },\n",
        "        'vehicle_model': {\n",
        "            'title': 'Vehicle Model',\n",
        "            'type': 'string',\n",
        "            'description': 'Model of the person\\'s vehicle'\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLkSz-PUH0nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c2cb13-546c-42fa-b8f5-c6279909c1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 0.2.0. Use create_structured_output_runnable instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Defining the LangChain chain object for function calling\n",
        "chain = create_structured_output_chain(output_schema = langchain_json_schema,\n",
        "                                       llm = llm,\n",
        "                                       prompt = chat_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lux3-94PH0nZ",
        "outputId": "d80b07f7-f85b-4430-849b-d6aa40b1eb41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my_prompt': 'I drive a Tesla Model 3', 'function': {'vehicle_make': 'Tesla', 'vehicle_model': 'Model 3'}}\n"
          ]
        }
      ],
      "source": [
        "# Getting results with a demo prompt\n",
        "print(chain.invoke(input = {'my_prompt': 'I drive a Tesla Model 3'}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TfxGNd9H0nZ"
      },
      "outputs": [],
      "source": [
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7svqhafB0Zzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}