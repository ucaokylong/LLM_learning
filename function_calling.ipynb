{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucaokylong/LLM_learning/blob/main/function_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivHdTOyFH0nT"
      },
      "source": [
        "# OpenAI Function Calling 101\n",
        "Má»™t trong nhá»¯ng khÃ³ khÄƒn khi sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) nhÆ° ChatGPT lÃ  chÃºng khÃ´ng táº¡o ra Ä‘áº§u ra dá»¯ liá»‡u cÃ³ cáº¥u trÃºc. Äiá»u nÃ y ráº¥t quan trá»ng Ä‘á»‘i vá»›i cÃ¡c há»‡ thá»‘ng láº­p trÃ¬nh phá»¥ thuá»™c pháº§n lá»›n vÃ o dá»¯ liá»‡u cÃ³ cáº¥u trÃºc Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c. VÃ­ dá»¥, náº¿u báº¡n muá»‘n xÃ¢y dá»±ng má»™t chÆ°Æ¡ng trÃ¬nh phÃ¢n tÃ­ch cáº£m xÃºc cá»§a má»™t bÃ i Ä‘Ã¡nh giÃ¡ phim, báº¡n cÃ³ thá»ƒ pháº£i thá»±c hiá»‡n má»™t Ä‘oáº¡n code trÃ´ng giá»‘ng nhÆ° sau:\n",
        "\n",
        "```\n",
        "prompt = f'''\n",
        "Please perform a sentiment analysis on the following movie review:\n",
        "{MOVIE_REVIEW_TEXT}\n",
        "Please output your response as a single word: either \"Positive\" or \"Negative\". Do not add any extra characters.\n",
        "'''\n",
        "```\n",
        "\n",
        "Váº¥n Ä‘á» lÃ  Ä‘iá»u nÃ y khÃ´ng pháº£i lÃºc nÃ o cÅ©ng hiá»‡u quáº£. KhÃ¡ phá»• biáº¿n lÃ  LLM sáº½ thÃªm vÃ o má»™t dáº¥u cháº¥m khÃ´ng mong muá»‘n hoáº·c giáº£i thÃ­ch dÃ i hÆ¡n nhÆ°: \"Cáº£m xÃºc cá»§a bá»™ phim nÃ y lÃ : TÃ­ch cá»±c.\" Máº·c dÃ¹ báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng biá»ƒu thá»©c chÃ­nh quy (regex) Ä‘á»ƒ láº¥y ra cÃ¢u tráº£ lá»i (ğŸ¤¢), nhÆ°ng rÃµ rÃ ng Ä‘Ã¢y khÃ´ng pháº£i lÃ  lÃ½ tÆ°á»Ÿng. Äiá»u lÃ½ tÆ°á»Ÿng sáº½ lÃ  náº¿u LLM tráº£ vá» káº¿t quáº£ dÆ°á»›i dáº¡ng cáº¥u trÃºc JSON nhÆ° sau:\n",
        "\n",
        "```\n",
        "{\n",
        "    'sentiment': 'positive'\n",
        "}\n",
        "```\n",
        "\n",
        "OpenAI Ä‘Ã£ giá»›i thiá»‡u má»™t tÃ­nh nÄƒng má»›i gá»i lÃ  function calling, giÃºp giáº£i quyáº¿t váº¥n Ä‘á» trÃªn. Function calling chÃ­nh lÃ  cÃ¢u tráº£ lá»i cho váº¥n Ä‘á» trÃªn. Jupyter notebook nÃ y sáº½ minh há»a má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n vá» cÃ¡ch sá»­ dá»¥ng function calling má»›i cá»§a OpenAI trong Python. Náº¿u báº¡n muá»‘n xem tÃ i liá»‡u Ä‘áº§y Ä‘á»§, [vui lÃ²ng xem liÃªn káº¿t nÃ y](https://platform.openai.com/docs/guides/gpt/function-calling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT1ePnTaH0nV"
      },
      "source": [
        "## Notebook Setup\n",
        "HÃ£y báº¯t Ä‘áº§u vá»›i cÃ¡c import. BÃ¢y giá», cÃ³ thá»ƒ báº¡n Ä‘Ã£ cÃ i Ä‘áº·t client Python `openai`, nhÆ°ng ráº¥t cÃ³ thá»ƒ báº¡n cáº§n nÃ¢ng cáº¥p nÃ³ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c chá»©c nÄƒng function calling má»›i. ÄÃ¢y lÃ  cÃ¡ch nÃ¢ng cáº¥p trong Terminal / Powershell cá»§a báº¡n báº±ng `pip`:\n",
        "\n",
        "```\n",
        "pip install openai --upgrade\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --upgrade"
      ],
      "metadata": {
        "id": "iN4qCK21hBor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzJqI_a6H0nV"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary Python libraries\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URyKfrF1H0nW"
      },
      "source": [
        "Äá»ƒ kiá»ƒm tra chá»©c nÄƒng function calling, tÃ´i Ä‘Ã£ viáº¿t má»™t Ä‘oáº¡n \"About Me\" ngáº¯n chá»©a cÃ¡c sá»± tháº­t cá»¥ thá»ƒ mÃ  chÃºng ta sáº½ phÃ¢n tÃ­ch thÃ nh cÃ¡c cáº¥u trÃºc dá»¯ liá»‡u phÃ¹ há»£p, bao gá»“m sá»‘ nguyÃªn vÃ  chuá»—i. HÃ£y táº£i vÄƒn báº£n nÃ y vÃ o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHNSQ593H0nW"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Loading the \"About Me\" text from local file\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "about_me = \"Hello! My name is David Hundley. I am a principal machine learning engineer at State Farm. I enjoy learning about AI and teaching what I learn back to others. I have two daughters. I drive a Tesla Model 3, and my favorite video game series is The Legend of Zelda.\"\n",
        "\n",
        "print(about_me)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W4Q38BcH0nX"
      },
      "source": [
        "## The Pre-Function Calling Days\n",
        "TrÆ°á»›c khi chÃºng ta minh há»a function calling, hÃ£y minh há»a cÃ¡ch chÃºng ta Ä‘Ã£ sá»­ dá»¥ng prompt engineering vÃ  Regex Ä‘á»ƒ táº¡o ra má»™t JSON cÃ³ cáº¥u trÃºc mÃ  chÃºng ta cÃ³ thá»ƒ lÃ m viá»‡c má»™t cÃ¡ch láº­p trÃ¬nh sau nÃ y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7u7WOlAH0nX"
      },
      "outputs": [],
      "source": [
        "# Engineering a prompt to extract as much information from \"About Me\" as a JSON object\n",
        "about_me_prompt = f'''\n",
        "Please extract information as a JSON object. Please look for the following pieces of information.\n",
        "Name\n",
        "Job title\n",
        "Company\n",
        "Number of children as a single number\n",
        "Car make\n",
        "Car model\n",
        "Favorite video game series\n",
        "\n",
        "This is the body of text to extract the information from:\n",
        "{about_me}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ276CVDH0nX"
      },
      "outputs": [],
      "source": [
        "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "openai_response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': about_me_prompt}]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXT-v6-bH0nX",
        "outputId": "9fbad469-04e0-4ffd-d988-e6efd6b1370b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Name': 'David Hundley', 'Job title': 'Principal Machine Learning Engineer', 'Company': 'State Farm', 'Number of children': 2, 'Car make': 'Tesla', 'Car model': 'Model 3', 'Favorite video game series': 'The Legend of Zelda'}\n"
          ]
        }
      ],
      "source": [
        "# Loading the response as a JSON object\n",
        "json_response = json.loads(openai_response.choices[0].message.content)\n",
        "print(json_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjGXw7QH0nX"
      },
      "source": [
        "## Using the New Function Calling Capabilities\n",
        "BÃ¢y giá» chÃºng ta Ä‘Ã£ minh há»a cÃ¡ch chÃºng ta Ä‘Ã£ tá»«ng láº¥y Ä‘Æ°á»£c JSON cÃ³ cáº¥u trÃºc trong nhá»¯ng ngÃ y trÆ°á»›c khi cÃ³ function calling, hÃ£y chuyá»ƒn sang cÃ¡ch chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng function calling Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c káº¿t quáº£ tÆ°Æ¡ng tá»± nhÆ°ng theo cÃ¡ch nháº¥t quÃ¡n hÆ¡n cho viá»‡c sá»­ dá»¥ng há»‡ thá»‘ng cá»§a chÃºng ta. ChÃºng ta sáº½ báº¯t Ä‘áº§u Ä‘Æ¡n giáº£n hÆ¡n vá»›i má»™t hÃ m tÃ¹y chá»‰nh duy nháº¥t vÃ  sau Ä‘Ã³ giáº£i quyáº¿t má»™t sá»‘ chá»©c nÄƒng \"nÃ¢ng cao\" hÆ¡n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWikJtaqH0nX"
      },
      "outputs": [],
      "source": [
        "# Defining our initial extract_person_info function\n",
        "def extract_person_info(name, job_title, num_children):\n",
        "    '''\n",
        "    Prints basic \"About Me\" information\n",
        "\n",
        "    Inputs:\n",
        "        name (str): Name of the person\n",
        "        job_title (str): Job title of the person\n",
        "        num_chilren (int): The number of children the parent has.\n",
        "    '''\n",
        "\n",
        "    print(f'This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNS7-6XqH0nX"
      },
      "outputs": [],
      "source": [
        "# Defining how we want ChatGPT to call our custom functions\n",
        "my_custom_functions = [\n",
        "    {\n",
        "        'name': 'extract_person_info',\n",
        "        'description': 'Get \"About Me\" information from the body of the input text',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person'\n",
        "                },\n",
        "                'job_title': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Job title of the person'\n",
        "                },\n",
        "                'num_children': {\n",
        "                    'type': 'integer',\n",
        "                    'description': 'Number of children the person is a parent to'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVKEN9tQH0nY",
        "outputId": "3d8585a6-035f-49db-dd0b-b8ddd6fd8db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-94o26OACU2zjAMUwNvIDsXvqnRCQn', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1710932666, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=30, prompt_tokens=147, total_tokens=177))\n"
          ]
        }
      ],
      "source": [
        "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "openai_response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': about_me}],\n",
        "    functions = my_custom_functions,\n",
        "    function_call = 'auto'\n",
        ")\n",
        "\n",
        "print(openai_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai_response.choices[0].message.function_call)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceI9oBI7iQWE",
        "outputId": "dfe7a6bc-bba6-467d-c587-5cb88cf2d7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7kQ2GsUH0nY"
      },
      "source": [
        "### What if the prompt I submit doesn't contain the information I want to extract per my custom function?\n",
        "Copy code\n",
        "Trong vÃ­ dá»¥ ban Ä‘áº§u cá»§a chÃºng ta, hÃ m tÃ¹y chá»‰nh Ä‘Ã£ tÃ¬m cÃ¡ch trÃ­ch xuáº¥t ba thÃ´ng tin ráº¥t cá»¥ thá»ƒ vÃ  chÃºng ta Ä‘Ã£ chá»©ng minh ráº±ng Ä‘iá»u nÃ y hoáº¡t Ä‘á»™ng thÃ nh cÃ´ng báº±ng cÃ¡ch truyá»n vÄƒn báº£n \"About Me\" tÃ¹y chá»‰nh cá»§a tÃ´i dÆ°á»›i dáº¡ng má»™t prompt. NhÆ°ng báº¡n cÃ³ thá»ƒ tá»± há»i, Ä‘iá»u gÃ¬ sáº½ xáº£y ra náº¿u báº¡n truyá»n vÃ o báº¥t ká»³ prompt nÃ o khÃ¡c khÃ´ng chá»©a thÃ´ng tin Ä‘Ã³?\n",
        "\n",
        "HÃ£y nhá»› láº¡i ráº±ng chÃºng ta Ä‘Ã£ Ä‘áº·t má»™t tham sá»‘ trong lá»‡nh gá»i API client cá»§a mÃ¬nh gá»i lÃ  function_call mÃ  chÃºng ta Ä‘áº·t thÃ nh auto. ChÃºng ta sáº½ khÃ¡m phÃ¡ Ä‘iá»u nÃ y sÃ¢u hÆ¡n ná»¯a trong pháº§n tiáº¿p theo, nhÆ°ng vá» cÆ¡ báº£n, tham sá»‘ nÃ y Ä‘ang yÃªu cáº§u ChatGPT sá»­ dá»¥ng phÃ¡n Ä‘oÃ¡n tá»‘t nháº¥t cá»§a nÃ³ Ä‘á»ƒ tÃ¬m ra khi nÃ o cáº§n cáº¥u trÃºc Ä‘áº§u ra cho má»™t trong cÃ¡c hÃ m tÃ¹y chá»‰nh cá»§a chÃºng ta.\n",
        "\n",
        "Váº­y Ä‘iá»u gÃ¬ sáº½ xáº£y ra khi chÃºng ta gá»­i má»™t prompt khÃ´ng khá»›p vá»›i báº¥t ká»³ hÃ m tÃ¹y chá»‰nh nÃ o cá»§a chÃºng ta? NÃ³i má»™t cÃ¡ch Ä‘Æ¡n giáº£n, nÃ³ sáº½ máº·c Ä‘á»‹nh trá»Ÿ láº¡i hÃ nh vi thÃ´ng thÆ°á»ng nhÆ° thá»ƒ function calling khÃ´ng tá»“n táº¡i. HÃ£y kiá»ƒm tra Ä‘iá»u nÃ y vá»›i má»™t prompt tÃ¹y Ã½: \"ThÃ¡p Eiffel cao bao nhiÃªu?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1UBGLPMH0nY",
        "outputId": "7abf4457-cc06-4668-9f62-6a1cea4c58b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-94o7fPVQErjmsLTu7ynKXW3BQNxdG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Eiffel Tower is 1,063 feet (324 meters) tall, including antennas.', role='assistant', function_call=None, tool_calls=None))], created=1710933011, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=21, prompt_tokens=97, total_tokens=118))\n"
          ]
        }
      ],
      "source": [
        "# Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "openai_response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': 'How tall is the Eiffel Tower?'}],\n",
        "    functions = my_custom_functions,\n",
        "    function_call = 'auto'\n",
        ")\n",
        "\n",
        "print(openai_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-BK6U8SH0nY"
      },
      "outputs": [],
      "source": [
        "# Defining a function to extract only vehicle information\n",
        "def extract_vehicle_info(vehicle_make, vehicle_model):\n",
        "    '''\n",
        "    Prints basic vehicle information\n",
        "\n",
        "    Inputs:\n",
        "        - vehicle_make (str): Make of the vehicle\n",
        "        - vehicle_model (str): Model of the vehicle\n",
        "    '''\n",
        "\n",
        "    print(f'Vehicle make: {vehicle_make}\\nVehicle model: {vehicle_model}')\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function to extract all information provided in the original \"About Me\" prompt\n",
        "def extract_all_info(name, job_title, num_children, vehicle_make, vehicle_model, company_name, favorite_vg_series):\n",
        "    '''\n",
        "    Prints the full \"About Me\" information\n",
        "\n",
        "    Inputs:\n",
        "        - name (str): Name of the person\n",
        "        - job_title (str): Job title of the person\n",
        "        - num_chilren (int): The number of children the parent has\n",
        "        - vehicle_make (str): Make of the vehicle\n",
        "        - vehicle_model (str): Model of the vehicle\n",
        "        - company_name (str): Name of the company the person works for\n",
        "        - favorite_vg_series (str): Person's favorite video game series.\n",
        "    '''\n",
        "\n",
        "    print(f'''\n",
        "    This person\\'s name is {name}. Their job title is {job_title}, and they have {num_children} children.\n",
        "    They drive a {vehicle_make} {vehicle_model}.\n",
        "    They work for {company_name}.\n",
        "    Their favorite video game series is {favorite_vg_series}.\n",
        "    ''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MCTQFNfH0nY"
      },
      "outputs": [],
      "source": [
        "# Defining how we want ChatGPT to call our custom functions\n",
        "my_custom_functions = [\n",
        "    {\n",
        "        'name': 'extract_person_info',\n",
        "        'description': 'Get \"About Me\" information from the body of the input text',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person'\n",
        "                },\n",
        "                'job_title': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Job title of the person'\n",
        "                },\n",
        "                'num_children': {\n",
        "                    'type': 'integer',\n",
        "                    'description': 'Number of children the person is a parent to'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'extract_vehicle_info',\n",
        "        'description': 'Extract the make and model of the person\\'s car',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'vehicle_make': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Make of the person\\'s vehicle'\n",
        "                },\n",
        "                'vehicle_model': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Model of the person\\'s vehicle'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'extract_all_info',\n",
        "        'description': 'Extract all information about a person including their vehicle make and model',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person'\n",
        "                },\n",
        "                'job_title': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Job title of the person'\n",
        "                },\n",
        "                'num_children': {\n",
        "                    'type': 'integer',\n",
        "                    'description': 'Number of children the person is a parent to'\n",
        "                },\n",
        "                'vehicle_make': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Make of the person\\'s vehicle'\n",
        "                },\n",
        "                'vehicle_model': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Model of the person\\'s vehicle'\n",
        "                },\n",
        "                'company_name': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the company the person works for'\n",
        "                },\n",
        "                'favorite_vg_series': {\n",
        "                    'type': 'string',\n",
        "                    'description': 'Name of the person\\'s favorite video game series'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7bKZqEnH0nY"
      },
      "source": [
        "Now let's demonstrate what happens when we apply 3 different samples to all of the custom functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ondbhtOmH0nY"
      },
      "outputs": [],
      "source": [
        "# Defining a list of samples\n",
        "samples = [\n",
        "    about_me,\n",
        "    'My name is David Hundley. I am a principal machine learning engineer, and I have two daughters.',\n",
        "    'She drives a Kia Sportage.'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNF0ss92H0nY",
        "outputId": "08dea244-f134-4886-8a5d-6dfe94574b77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample #1's results:\n",
            "ChatCompletion(id='chatcmpl-94o8LMn34OQ4Nw6nG7Aq12ehlMhjG', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2,\"vehicle_make\":\"Tesla\",\"vehicle_model\":\"Model 3\",\"company_name\":\"State Farm\",\"favorite_vg_series\":\"The Legend of Zelda\"}', name='extract_all_info'), tool_calls=None))], created=1710933053, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=58, prompt_tokens=320, total_tokens=378))\n",
            "Sample #2's results:\n",
            "ChatCompletion(id='chatcmpl-94o8NGEFk16kB8YZvuJpk3c5cl5JN', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"David Hundley\",\"job_title\":\"Principal Machine Learning Engineer\",\"num_children\":2}', name='extract_person_info'), tool_calls=None))], created=1710933055, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=30, prompt_tokens=282, total_tokens=312))\n",
            "Sample #3's results:\n",
            "ChatCompletion(id='chatcmpl-94o8OzN5cpUhTNZ2dplMqMyZma0uu', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"vehicle_make\":\"Kia\",\"vehicle_model\":\"Sportage\"}', name='extract_vehicle_info'), tool_calls=None))], created=1710933056, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=23, prompt_tokens=268, total_tokens=291))\n"
          ]
        }
      ],
      "source": [
        "# Iterating over the three samples\n",
        "for i, sample in enumerate(samples):\n",
        "\n",
        "    print(f'Sample #{i + 1}\\'s results:')\n",
        "\n",
        "    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "    openai_response = openai.chat.completions.create(\n",
        "        model = 'gpt-3.5-turbo',\n",
        "        messages = [{'role': 'user', 'content': sample}],\n",
        "        functions = my_custom_functions,\n",
        "        function_call = 'auto'\n",
        "    )\n",
        "\n",
        "    # Printing the sample's response\n",
        "    print(openai_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD7qtImGH0nY"
      },
      "source": [
        "With each of the respective prompts, ChatGPT selected the correct custom function, and we can specifically note that in the `name` value under `function_call` in the API's response object. In addition to this being a handy way to identify which function to use the arguments for, we can programmatically map our actual custom Python function to this value to run the correct code appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGfDFvrTH0nZ",
        "outputId": "fd7e6f53-2bcb-45ba-d8dd-0847933bf1a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample #1's results:\n",
            "\n",
            "    This person's name is David Hundley. Their job title is Principal Machine Learning Engineer, and they have 2 children.\n",
            "    They drive a Tesla Model 3.\n",
            "    They work for State Farm.\n",
            "    Their favorite video game series is The Legend of Zelda.\n",
            "    \n",
            "Sample #2's results:\n",
            "This person's name is David Hundley. Their job title is Principal Machine Learning Engineer, and they have 2 children.\n",
            "Sample #3's results:\n",
            "Vehicle make: Kia\n",
            "Vehicle model: Sportage\n"
          ]
        }
      ],
      "source": [
        "# Iterating over the three samples\n",
        "for i, sample in enumerate(samples):\n",
        "\n",
        "    print(f'Sample #{i + 1}\\'s results:')\n",
        "\n",
        "    # Getting the response back from ChatGPT (gpt-3.5-turbo)\n",
        "    openai_response = openai.chat.completions.create(\n",
        "        model = 'gpt-3.5-turbo',\n",
        "        messages = [{'role': 'user', 'content': sample}],\n",
        "        functions = my_custom_functions,\n",
        "        function_call = 'auto'\n",
        "    ).choices[0].message\n",
        "\n",
        "    # Checking to see that a function call was invoked\n",
        "    if openai_response.function_call:\n",
        "        # Checking to see which specific function call was invoked\n",
        "        function_called = openai_response.function_call.name\n",
        "\n",
        "        # Extracting the arguments of the function call\n",
        "        function_args = json.loads(openai_response.function_call.arguments)\n",
        "\n",
        "        # Invoking the proper functions\n",
        "        if function_called == 'extract_person_info':\n",
        "            extract_person_info(*list(function_args.values()))\n",
        "        elif function_called == 'extract_vehicle_info':\n",
        "            extract_vehicle_info(*list(function_args.values()))\n",
        "        elif function_called == 'extract_all_info':\n",
        "            extract_all_info(*list(function_args.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpXnf2mAH0nZ"
      },
      "source": [
        "## OpenAI Function Calling with LangChain\n",
        "Given its popularity amongst the Generative AI community, I thought I'd re-visit this notebook and add some code to show how you might make use of this exact same functionality in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhOmubQskrv6",
        "outputId": "5b0962ac-edd0-4581-9b50-6d40ada19a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.28 (from langchain)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.14.2)\n",
            "Collecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.31->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, tiktoken, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-openai-0.0.8 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zivmk4LtH0nZ"
      },
      "outputs": [],
      "source": [
        "# Importing the LangChain objects\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain.chains.openai_functions import create_structured_output_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp067ipkH0nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f1f564-6648-4ec7-bfa9-4c90fb0567f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Setting the proper instance of the OpenAI model\n",
        "llm = ChatOpenAI(model = 'gpt-3.5-turbo-0613')\n",
        "\n",
        "# Setting a LangChain ChatPromptTemplate\n",
        "chat_prompt_template = ChatPromptTemplate.from_template('{my_prompt}')\n",
        "\n",
        "# Setting the JSON schema for extracting vehicle information\n",
        "langchain_json_schema = {\n",
        "    'name': 'extract_vehicle_info',\n",
        "    'description': 'Extract the make and model of the person\\'s car',\n",
        "    'type': 'object',\n",
        "    'properties': {\n",
        "        'vehicle_make': {\n",
        "            'title': 'Vehicle Make',\n",
        "            'type': 'string',\n",
        "            'description': 'Make of the person\\'s vehicle'\n",
        "        },\n",
        "        'vehicle_model': {\n",
        "            'title': 'Vehicle Model',\n",
        "            'type': 'string',\n",
        "            'description': 'Model of the person\\'s vehicle'\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLkSz-PUH0nZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c2cb13-546c-42fa-b8f5-c6279909c1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 0.2.0. Use create_structured_output_runnable instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Defining the LangChain chain object for function calling\n",
        "chain = create_structured_output_chain(output_schema = langchain_json_schema,\n",
        "                                       llm = llm,\n",
        "                                       prompt = chat_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lux3-94PH0nZ",
        "outputId": "d80b07f7-f85b-4430-849b-d6aa40b1eb41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my_prompt': 'I drive a Tesla Model 3', 'function': {'vehicle_make': 'Tesla', 'vehicle_model': 'Model 3'}}\n"
          ]
        }
      ],
      "source": [
        "# Getting results with a demo prompt\n",
        "print(chain.invoke(input = {'my_prompt': 'I drive a Tesla Model 3'}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TfxGNd9H0nZ"
      },
      "outputs": [],
      "source": [
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7svqhafB0Zzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}